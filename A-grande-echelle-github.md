À grande échelle 
Comment les “Big data” transforment les sciences humaines

# 4e de couverture

Nous avons fait l’expérience dans les dix dernières années de formidables changements d’échelles. Alors qu’au XXe siècle, les sciences humaines et sociales devaient construire avec patience et rigueur les données pour leurs études, elles se retrouvent au XXe devant à un déluge informationnel d’une ampleur sans précédent. Leurs pratiques et leurs méthodes s’en trouvent nécessairement modifiées. Grâce aux progrès des sciences de l’information, des perspectives inédites s’ouvrent accompagnées de nouveaux questionnements. Peut-on écrire l’histoire différemment quand des kilomètres d’archives peuvent être numérisés ? Peut-on utiliser les dizaines de millions de livres numérisés pour faire un autre genre d’études littéraires ? Comment étudier les effets des algorithmes sur la langue ? Que nous disent les centaines de millions de nouvelles photos postées chaque jour sur Internet ? Peut-on vraiment enseigner simultanément à des dizaines de milliers d’étudiants ?

Ce livre dessine les contours de la grande transformation qui touche aujourd’hui les sciences humaines et sociales alors qu’elles intègrent les algorithmes comme des outils de travail et de médiation. Il montre par exemple à quoi pourraient ressembler une linguistique des grands nombres, des sciences historiques capables de produire des « Google maps » du passé ou une sociologie se constituant en temps réel à partir de flux échangés sur les réseaux sociaux. 

Ce livre explore aussi bien les promesses que les risques de cette transformation des sciences de l’homme et trace les contours de nouvelles aventures scientifiques, aussi vertigineuses que les grandes mutations culturelles de notre époque. 


La reconquête numérique du passé. 

#  Prologue : Des robots à Venise

Venice. Mai 2014 

L’entrée des robots.
Il y a dix ans. / Presentation 

Arrivée des scanner. Bientôt 4000 pages par jour. Le passé est la nouvelle frontière

#  Introduction : Le grand damier. 

Imaginez un plan immense, organisé comme un damier, constitué d’un très grand nombre de petites cases. Chacune de ses représente une capacité de stockage mémoire, en l’occurrence un kilo-octet, 1024 caractères. Un de ces carrés peut contenir une demi-page de texte d’un livre comme celui que vous tenez entre les mains. 

Commençons à nous élever dans les airs pour voir cet immense damier d’un peu plus haut. Il nous faudra monter un peu haut pour voir le nombre de cases nécessaires à accueillir un livre comme celui-ci, environ 600 cases. 

Continuous de monter, nous rejoignons rapidement un autre plan ou chaque case représente maintenant une capacité d’un méga-octet, soit 1024 kilo-octets. Le texte d’un livre de 500 pages tiens dans un seul de ces carrés. Un autre carré peut accueillir une photo couleur de qualité moyenne ou une minute d’un fichier mp3. La disquette des débuts de la micro-informatique pouvant accueillir  1.4 mega, permettait de stocker un peu plus que le contenu d’une des case de ce nouveau damier. Dans dix de ces cases, je peux stocker une minute de son en haute fidélité ou une ou deux minutes d’une video compressée. 

Continuons notre ascension.  Nous pouvons maintenant voir 650 de ces cases, la capacité moyenne d’un CD-rom et arrivons bientôt un nouveau plan, au chaque case peut maintenant contenir 1 Giga-octet, soit 1024 Mega-octets. Deux cases de ce nouveau damiers permettent de stocker une heure de video de bonne qualité. Un DVD correspond a un peu moins de que 5 cases de ce plan, un DVD blu-ray peut stocker jusqu’au 200 de ces cases. 

Le prochain plan est celui où chaque case représente un teraoctet, 1024 giga-octets. Un téraoctet correspond à la taille d’un ordinateur moyen en 2012. Dans une case nous pouvons stocker le texte d’environ un million de livres, soit le nombre de nouveaux livres produit à chaque année dans le monde, à 500 heures de films videos en codage MPEG-2. C’est aussi la capacité nécessaire pour enregistrer toutes les conversations d’une personne dans une une vie en codage MP3, soit environ 20 000 heures. Avec dix de ces cases, je peux stockers environ 10 millions de photos, soit l’équivalent de quantité  d’image partagées par heure sur Facebook. Avec seulement une vingtaine de cases de ce damier je peux stocker tous les textes de la bibliothèque du Congrès de Washington ou de 20 millions d’ouvrages numérisés dans le cadre du programme Google Books. Il me faudra 500 cases de ce damier pour stocker le film complet d’une vie. 

Dans le plan suivant, celui des petaoctets, chaque case à une capacité 1024 fois supérieures. Deux cases de ce damiers peuvent  accueillir l’ensemble des archives de net tel qu’il était en 2012. Il faut quinze cases pour stocker les données produite chaque années par le LHC au CERN à Genève. 

1 petaoctet = 1024 teraoctet
2 petaocter = les archives du net en 2012
3 petaoctets = la bibliothèque du Congres en comptant livres, cartes, photos, enregistrement, films, etc
15 petaoctets = la quantités de donnée produite chaque année par le LHC au CERN du Genève

1 exaoctet = 1024 petaoctets
= capacité tolite en ordre de grandeur des bases de données de Google (pour son moteur de recherche, gmail ou de Facebook)
= capacité mémoire totale des chevreaux humaines (d’après Laundauer !)

100 exaocter
= toute l’information papier sur terre
= toute l’information sous forme de bandes magnétique analgoqoiue : cassette audio, video 

400 exaoctets
= debut Internet Mondial (Evalué par Cisco qui prévoit un accroissement d’un facteur 4 dans les cinq années à venir)

1 zettaocteru = 1024 capacité de stockage numéroqiued el’humanitues (les disque durs) en 2012. Elle croitra d’un facteur 10 tous les cinq ans. 

——

Croissance de l’ordinateur mondial, utilisé pour des milliards d’invididu. 

L’infomration numérique a dépassé l’information papier à un moment entre 2000 et 2010. 

L’échelle à changé. 




500 livres de tailles moyenne sans images

#  Comment construire une machines à remonter le temps

~~Les utilisateurs de Facebook partagent environ 10 millions de nouvelles photos par heure. Sur YouTube, une nouvelle heure de vidéo est mise en ligne par seconde. Chaque jour, près de 400 millions de nouveaux tweets  documentent minute par minute la pulsation du monde. Le système informationnel que ces différents flux irriguent ne se cesse de croitre, constituant une base documentaire sans précédent sur notre époque.  Ce déluge de données a ouvert un champ de recherche nouveau: les “Big data”. Stocker, indexer et repérer des structures dans ces flux est un des enjeux majeurs des premières décennies du XXIe siècle~~. 
~~
~~~~Mais cette augmentation exponentielle du volume des données  est aussi un facteur de déstabilisation culturelle. Nous produirions aujourd’hui plus de données en 2 jours que l’ensemble des données numériques disponibles il y a dix ans. Chaque jour, l’Internet ressemble un peu plus à un panoptique nous donnant à voir un “grand maintenant”, complexe, infiniment dense et parfaitement documenté, mais qui sans la disponibilité de données numériques équivalentes  sur le passé ne s’inscrit plus dans une temporalité longue. Ce déséquilibre est inquiétant, car une société ne saurait envisager avec sagesse son futur si elle n’est pas capable de comprendre sa trajectoire sur la durée~~. 
~~
~~~~Pourtant les données sur le passé existent et en très grande quantité. Les grands empires, mais aussi les états plus petits, se sont presque toujours construits grâce à la mise en place de politiques de traçabilité informationnelle consignant avec le plus de précision possible, non seulement la cartographie de leur territoire, mais aussi la vie de chaque habitant de leurs cités. Dans les grandes archives européennes, des milliers de kilomètres de registres regroupant les informations minutieusement enregistrées par ces “Google” du passé attendent d’être analysés. À ces archives  s’ajoutent des constellations de documents privés, dont la volumétrie est difficile à évaluer, mais qui constituent un contrepoint informationnel précieux à la mémoire administrative. Depuis quelques années, plusieurs initiatives pour numériser, transcrire, indexer massivement ce matériel historique ont vu le jour. Il s’agit de développer les technologies et les savoir-faire pour produire à partir de ces archives, des “Big data du passé”. L’enjeu de ces recherches est immense : il consiste à rendre enfin accessible des milliards d’informations qui échappent aujourd’hui à la culture numérique et donc peut-être à moyen terme, à la culture tout court. Pour les technologies de l’information, la reconquête numérique du passé est la prochaine frontière~~. 


## Rendre le passé présent

En 1896, à Genève, les deux millions de visiteurs de l’exposition nationale pouvaient pour la première fois découvrir leur ville depuis le ciel. Par petits groupes, depuis la nacelle d’un ballon à hélium, ils voyaient comment la ville s’était considérablement transformée dans les dernières décennies. Dans le sillage des autres capitales européennes en cette fin du XIXe siècle, l’ancienne cité médiévale était maintenant devenue une ville ouverte, aérée, cosmopolite, construite pour la promenade plus que pour la défense, une ville de son temps. Cette vue du ciel, absolument inédite à une époque ou les voyages aériens n’était encore qu’un rêve d’avenir, permettait d’un seul coup d’œil d’embrasser la structure et les motifs de la ville nouvelle. Elle donnait à voir un point de vue et une compréhension qu’aucun discours n’aurait pu résumer. 

Essayons d’imaginer l’expérience de ces visiteurs de la fin du XIXe siècle, peu coutumiers aux vues aériennes et qui pour beaucoup montaient pour la première fois dans un ballon. L’ivresse du panorama provient d’une transformation soudaine du rapport entre l’espace et le temps, une compression spatiotemporelle. D’un point unique, il devient tout d’un coup possible d’entrevoir sous une nouvelle forme des lieus qu’il fallait des heures à atteindre. La surabondance soudaine de l’information disponible donne le vertige. Où regarder ? L’œil cherche désespérément des motifs globaux ou au contraire un détail singulier pour rendre intelligible ce spectacle trop riche, trop dense, pour le faire revenir dans l’ordre du discours, pour être capables une fois redescendus de dire ce que l’on a vu. Mais le panorama ne laisse que très partiellement conter. Une fois en bas, les visiteurs devront en général se contenter de parler de l’expérience vertigineuse en tant que telle plutôt que de son contenu. 

Toujours à l’exposition universelle de Genève, mais sur la terre ferme cette fois-ci, comme un écho à ce premier vertigineux panorama, un autre type de spectacle était proposé dans un pavillon du parc de Plaisance. Il s’agissait d’une grande maquette de près de 30 m2, représentant Genève en 1850 et intitulée « Grand Relief du Vieux Genève ». Malgré les grandes dimensions de ce modèle en trois dimension, l’ancienne cité était reconstruite avec une grande minutie. Les visiteurs qui surplombaient la maquette étaient de nouveau placés dans un situation de vue panoramique, mais cette fois-ci face la reconstitution artificielle d’un paysage disparu. 

L’auteur de la maquette, Auguste Magnin (1841-1903), avait passé près de 18 ans à minutieusement construire cette modèle dans les combles de son atelier d’architecte. L’ensemble pesait près 670 kilos. Pour ce projet, il avait d’abord réuni une grande documentation, dessiné des relevés sur le terrain, décalqué des cartes et des vues de la ville et enfin dressé des plans définitifs de chaque groupe de maison. Magnin avait effectué des premiers modèles en carton pour évaluer la complexité de la tache et dimensionner son projet, puis avait progressivement convergé vers une combinaison de procédés pour construire la maquette. Elle reposait sur une structure de bois soutenant un assemblage de 120 caissons, puis d’une modélisation du terrain grâce à des planchettes d’un centimètre d’épaisseur. Façades et murs étaient bâtis en tôles de zinc soudées. Les tuiles des toits et les pavés des rues étaient imités par galvanoplastie. Par la gravure à l’acide, il avait pu reproduire le pavement des murs des fortifications. Les 1500 arbres de la maquette était en fonte, tous différents. Des variations dans la disposition des branches permettaient de suggérer la forme d’un platane, d’un peuplier ou d’un sapin. Magnin avait choisi de garder le zinc et le cuivre dans leur couleur naturelle, privilégiant avant tout l’esthétique et le rendu global de la maquette plutôt que de poursuivre une impossible tentative de réalisme. 

A partir des archives, des relevés de terrains et d’ancienne carte, Magnin avait du reconstruire la structure sous-jacente de la Genève de 1850, comprendre la grammaire architecturale et urbaine et choisir ensuite les procédés techniques pour recréer ce qui finalement une simulation de la ville disparue. A chaque étape il avait effectué de multiples choix. Pour accroitre la lisibilité du modèle recrée, Magnin n’avait pas hésiter à travailler sur trois échelles différentes : 1/250e pour le plan, le 1/200e pour l’élévation et le 1/100 pour le terrain. Si Magnin avait gardé une échelle constante, la ville se serait trouvée écrasée. En effectuant une distorsion généralisée des hauteurs, la maquette de Magnin gagnait en efficacité visuelle, rendait mieux compte des détails de la ville, même si par principe elle perdait en fidélité et cessait d’être un modèle absolument réaliste de la ville ancienne._

Magnin était précisément motivé pour s’atteler à cette œuvre de long allène par les grands bouleversements urbanistiques qui étaient en train d’affecter Genève dans cette seconde moitié du XIXe. Genève se voyait dotée d’une structuration nouvelle. Une nouvelle grammaire urbaine allait recouvrir la Cité ancienne. Magnin choisit la date symbolique de 1850, car elle correspondait aux premiers coups de pioche portés aux fortifications de la ville huguenote, le début de la fin d’un monde. Pour lui, il ne s’agissait pas tant d’en enregistrer les traces que de la redonner à voir, de la manière la plus impressionnante et la plus pédagogique et utilisant la technologie de son temps. Le panorama de l’ancienne Genève devait être aussi spectaculaire que celui de la Cité nouvelle vue du ciel. Travail d’un seul homme qui luttait pour faire vivre le passé dans un monde en pleine mutation, le relief Magnin porte en son sein les ambiguïtés du travail de reconstruction historique à fine granularité, une question qui est, comme nous allons le voir, éminemment contemporaine.  Le relief Magnin n’était pas une reproduction, mais une réinvention informée du passé, une tentative pour rendre le passé présent. 

## L’interface planétaire

En moins de dix ans notre rapport à l’espace s’est considérablement modifié. En quelques clics nous pouvons aujourd’hui depuis une vue distante de la Terre, assez semblable aux premières images prises par les astronautes de la mission Apollo, zoomer et voir à quoi ressemble une région comme si nous étions un oiseau en train de la survoler. Souvent nous pouvons également observer à quoi ressemble un bâtiment depuis une rue avoisinante et littéralement nous déplacer comme si nous étions sur place. Il ne fallut que quelques années, à l’espace urbain planétaire pour devenir un espace algorithmique, cartographié, photographié, articulé pour être explorable par l’intermédiaire des interfaces de nos ordinateurs. Le globe terrestre n’a pas été seulement mis en image, il a été “machinisé”. 

Les conséquences profondes de ces fantastiques dispositifs de vision sont encore à étudier. Mais une remarque s’impose d’emblée : le temps semble singulièrement invisible de ce dispositif.  Est-ce vraiment le présent que me donne à voir le globe algorithmique ? De quand datent ces images aériennes que me montre l’interface ? Ont-elles été prises au même moment que celles qui ont servi à construire la navigation immersive qui me permet maintenant de voir le même bâtiment de profil ?  Je regarde cette image prise d’une ville à vol d’oiseau, puis je plonge dans une de ces rues sans savoir si cette nouvelle prise de vue a été capturée la même année que la vue aérienne. En fait, en y réfléchissant, je sais bien que ces deux manières de documenter et représenter l’espace ne sont pas synchronisées. Je n’ai aucune idée de quand datent les images prises par le satellite ou l’avion qui permettent de voir la ville par dessus, ni quand la voiture qui collecte les images des facades des bâtiments est passée dans ces rues. Nous passons d’un dispositif de vision à l’autre, en toute continuité, comme si nous vivions dans un utopique perpétuel présent. 

Ce travestissement du temps  semble être une caractéristique propre des systèmes informationnels qui se sont développés dans la première décennie du XXIe siècle. Le deluge informationnel  dont nous faisons depuis quelques années l’expérience a conduit chaque acteur a parer au plus pressé : organiser l’information du monde, la catégoriser, l’articuler pour former de grands systèmes cartographiques et de grandes bases de données capables de mettre de l’ordre dans le chaos. A la possibilité de tracer en temps réel le déplacement des hommes et des marchandises, les échanges de messages, les transactions financières, le développement des systèmes de captation audiovisuelle généralisée de la vidéosurveillance à l’imagerie satellitaire, s’est ajouté l’opportunité pour chaque possesseur de dispositif informatiques portables de documenter sa propre vie comme jamais cela ne fut possible au paravant. Chacune de ces informations captées est potentiellement indexée dans le temps et l’espace, s’integrant dans un système de coordonnées en quatre dimensions, mais toujours selon une logique de stabilité de nos modes de représentation. Pourtant, rien n’est moins stable dans le temps que les modes d’organisation de l’information.  Pour s’en convaincre, il suffit de visiter une archive très ancienne.

## Le Google du moyen-âge

Ceux qui ont le privilège de visiter les archives d’état de Venise éprouvent presque toujours, la première fois, un sentiment de vertige. Auraient-ils pu imaginer  que derrière cette modeste porte du _Campo dei Frari_ pouvait se cacher un dédale de salles et de couloir habitant près 80 kilometres de documents anciens. Sur les étagères, pas des livres précieux mais des millions de registres administratifs, compte rendu politiques, déclarations d’impôts, contrats de travail, testaments, rapport de procès, plan cadastraux. La République de Venise a très tôt adopté une politique visant à garder trace des mouvements et  transactions de toute nature, accumulant un trésor informationnel sans équivalent. Venise était le Google du moyen-âge. 

Parcourant pour la première fois ces interminables couloirs, le visiteur se voit naturellement comme un archéologue qui aurait dans plusieurs siècle découvert les *data center* de notre époque, et qui réaliserait que ces racks d’ordinateurs contiennent une densité informationnelle suffisante pour refaire naitre la vie quotidienne de notre civilisation. Les documents des archives vénitiennes couvrent mille ans d’histoire. Ils ont gardé la trace des trajectoires biographiques de millions d’individus, de chaque transformation urbaine, de chaque épisode politique.  Il ne fait pas de doute qu’une science historique d’un nouveau genre pourrait naître d’une exploitation systématique de ces “Big data du passé”.

Pour cela il faudrait que ces informations aujourd’hui consignées des millions dans de documents manuscrits, écrits en dialecte vénitien, en latin ou en toscan puissent être accessibles par informatique, cherchables, indexables, visualisables comme les autres grandes bases de données auxquelles l’Internet nous donne accès. Songez qu’aujourd’hui le chercheur qui se rend à l’Archive ne peut demander que quelques documents par jour, qu’il n’a aucun moyen de trouver tous les documents faisant référence à une personne, un lieu ou un évènement particulier. Seule une longue enquête et une connaissance très fine de l’historique et de la structuration de l’archive peuvent lui permettre dans certain cas de reconstruire une partie du puzzle que constitue la réinvention du passé. L’enjeu de l’utilisation des technologies de l’information n’est pas seulement de rendre les documents accessibles mais surtout les connecter les uns avec les autres, de montrer comment ils s’articulent pour constituer un système et de permettre l’exploration agile de ce réseau documentaire. 

Pour transformer ces millions de document en un système d’information, numériser  ne serait pas suffisant. Il faudrait également  que les machines réussissent d’une certaine manière à les transcrire, puis à en extraire des informations structurées, indexant par exemple tous les documents parlant d’une certaine personne ou d’un certain lieu et identifiant les liens qui unissent ces “entités” les unes avec avec autres. Il faudrait également pouvoir replacer ces informations dans le temps et l’espace, recréer des cartes  représentant la situation non seulement de la ville, mais aussi de l’ensemble du bassin Méditerranéen au fil des années, utilisant l’énorme quantité d’information présent dans les documents des archives pour reconstituer le contexte nécessaire à l’interpretation de ces documents. 

Si nous étions effectivement face à des “Big data du passé”, pourrait-on imaginer de prolonger les services les plus populaire de l’Internet pour leur redonner ce qui leur manque cruellement : la durée, le temps long ? Pourrait-on imaginer un “Google map du passé” équipé d’une “slider” nous permettrait de voir le même lieu 50, 100 ou 1000 ans plus tôt ? Pourront-on imaginer de reconstituer un “Facebook du passé”, reproduisant le liens qui unissent des millions de personnes au moyen-âge, chroniquant leur vie avec une densité équivalente à celle qui caractérise nos récits de vie aujourd’hui. Peut-être que construire une machine à remonter le temps aujourd’hui voudrait dire exactement cela : rendre le passé aussi présent que le présent, l’intégrer dans le système d’information global. 

## Le champignon informationnel

Une manière de raisonner sur ce rêve peut-être impossible est d’envisager la quantité d’information numérique que nous avons sur chaque époque comme un champignon. Si nous plaçons verticalement le temps et horizontalement la quantité d’information disponible, le déluge informationnel des dix dernières années se visualise comme un grand plateau horizontal posé sur une base qui ne cesse de se rétrécir au fur et à mesure que nous descendons dans le passé. Pour construire un Google map ou un Facebook du passé, il nous faut d’une certaine manière transformer ce champignon en un rectangle, élargir sa base pour la rendre comparable à la taille du tableau, obtenir d’une manière ou d’une autre une densité informationnelle aussi importante pour le passé que pour le présent.

Une première approche consiste à conduire de vastes projets de numérisation, puis d’extraction systématique d’information dans ces collections numérisés. Le projet Google books, dont nous parlerons dans le prochain chapitre,  a ainsi numérisé près de 30 millions de livres et transcrit avec des logiciels de lecture automatique une grande partie d’entre eux, les rendant indexables et cherchables. Cette base, même si elle est constituée d’éléments extrêmement disparates (quoi de plus différent d’un livre qu’un autre livre) constitue néanmoins une source très riche d’information. Les grands projets de numérisation de la presse vont dans le même sens, permettant d’obtenir pour chaque jour une information détaillée sur les événements locaux et internationaux, mais également les cours de la bourse, les horaires de trains, les petites annonces, etc. La numérisation des données administratives donne des informations structurées de manière systématiques documentant les naissances, les décès, les mariages, les testaments, les changements de propriétés ou les modifications cadastrales. Les archives privées, extrêmement nombreuses viennent s’ajouter à cette déjà très riche collection d’information avec des photographies et des lettres. 

La logique archivistique et documentaire mise en place au début du XIXe siècle en France ou en Italie par exemple a encore de nombreux point commun avec celle qui à l’oeuvre aujourd’hui. Pour cette raison, il ne fait pas de doute que sur les deux cents ans dernières années en croissant par exemple les informations extraites des articles de presse et des administration, un tableau relativement précis de la société d’un pays et de son évolution au jour le jour puisse être reconstituée. Evidemment, plus nous reculons dans le passé, plus le nombre de documents se réduit et surtout plus les logiques de représentation ne deviennent étrangères. Les structures documentaires mis en place à la Renaissance, dans la lignée de la diffusion de l’imprimerie demandent un travail spécifique pour être interprètés selon les logiques des systèmes informationnels contemporains. La situation documentaire au moyen-âge et dans les périodes plus ancienne pose évidemment de nombreux autres défis. 

Pour compenser le manque d’information archivistique ou pour completer les espaces non couvert par les archives, nous pouvons adopter une stratégie complémentaire qui n’est pas étrangère à l’historien en reconstituant les données manquantes par extrapolation et généralisation. Si un historien retrouve le carnet de bord d’un capitaine de navire du XVIe siècle documentant précisément un voyage entre Venise et Corfou, il ne se contentera pas seulement d’en tirer des conclusions sur ce voyage particulier mais pourra, sous certaines conditions, utiliser ce document pour déduire des informations sur les modes de navigation et de vie maritime de cette époque. De la même manière, nous pouvons nous servir des informations architecturales d’un palazzo vénitien de style gothique encore très bien conservé pour faire des hypothèses sur la structure et le style d’un bâtiment de la même époque et ayant des fonctions similaires mais si ce dernier est aujourd’hui complètement détruit. Il s’agit toujours au delà de l’information contenu dans un document particulier, d’extraire des structures, des grammaires et de s’en servir pour construire des hypothèses motivées nous permettant de completer les blancs laissé entre les archives. En informatique, ces techniques correspondent à la grande famille des méthodes de simulation, étudiées depuis plus de 50 ans.  La problématique de la découverte de structures dans des données bruitées, de la généralisation à partir d’exemple, de l’extrapolation suivant certaines hypothèses de travail constitue la base de toutes une science qui s’est développée en parallèle des sciences historiques.  Les “Big data du passé” annoncent peut-être une interface féconde et nouvelle entre ces domaines et invitent en tout cas à réfléchir au statut épistémologique singulier de ces “passés reconstitués”. 

La simulation ne saurait être comprise comme une manière de simplement compenser le manque de données historiques, utilisées seulement quand les données archivistiques manquent. En fait il n’y a jamais assez de données. Aucun plan cadastral, aucune photo, aucun relevé laser, ne pourrait me permettre de reconstituer précisément la structure d’une seule _calle_ de Venise. Même dans des situations d’”hyperdocumentation”, il faut à un moment ou un autre prolonger les données selon certaines hypothèses. Simulations et données vont toujours de paire. Seule la résolution et l’incertitude change. 

## Un bien commun scientifique 

Encore aujourd’hui la plupart des historiens ont l’habitude de travailler en toutes petites équipes, se focalisant sur des problématiques très spécifiques. Ils n’échangent que très rarement leurs notes ou leurs données, percevant à tort ou à raison que leurs travaux de recherche préparatoire sont à la base de l’originalité de leurs travaux futurs. Prendre conscience de la dimension et la densité informationnelle des archives comme celle de Venise doit nous faire réaliser de l’impossibilité pour quelques historiens, travaillant de manière non coordonnée de couvrir avec une quelconque systématicité un objet aussi vaste. Si nous voulons tenter de transformer une archive de 80 kilomètres couvrant mille ans d’histoire en un système d’information structuré il nous faut développer un programme scientifique collaboratif, coordonné et massif. Nous sommes devant une entité informationnelle trop grande. Seule une collaboration scientifique internationale peut tenter d’en venir à bout.

Ce n’est pas la première fois que des disciplines se trouvent devant à une difficulté de ce type. La transformation des archives de Venise en système d’information ressemble aux défis que les chercheurs en rencontré quand ils ont tenté de modéliser des systèmes très vastes comme le système génomique, le système cérébral ou le système planétaire. Pour constituer les bases de données nécessaires à l’étude du Génome, le Cerveau ou la Terre, les chercheurs de disciplines pourtant très compétitives ont réussi à mettre en place des programmes internationaux coordonnés. Ces programmes ont permis la création d’un bien commun scientifique, utilisable librement par tous les chercheurs du monde et jouant un rôle de pivot pour l’élaboration d’une connaissance construite collectivement. La même approche doit être développée quand nous envisageons des programmes de recherches en sciences humaines et sociales à grande échelle. 

*Note : Le projet “Venice Time Machine” dont il est question dans ce chapitre s’est d’abord articulé autour de deux partenaires principaux l’École Polytechnique Fédérale de Lausanne et l’Université Ca’Foscari à Venise. L’ambition pour ces deux partenaires initiaux était de mettre en place les structures et la méthodologie de travail pour constituer la plus grande base de données jamais créée sur Venise et son empire en se basant en premier lieu sur un programme de numérisation massive des archives des la cité des doges. Pourrait ensuite se joindre un nombre croissant d’équipes de recherche travaillant sur Venise venant du monde entier. Chacun des travaux de recherche de ces équipes pourraient bénéficier de l’information produite par les autres et à leur tour enrichir la base*.

Pour mettre en place un régime collaboratif permettant la création d’un bien commun scientifique il faut relever un certain nombre de défis. Il faut d’abord convaincre les archives que leur mission de conservation et de valorisation d’un patrimoine millénaire peuvent prendre la forme d’une libération sous forme numérique des images des documents dont elles assurent la sécurité et la transmission depuis tant d’années. Une image n’est pas le document lui-même, mais leur mise à disposition sur l’Internet peut dispenser les chercheurs de se rendre physiquement à l’archive, ce qui peut tendre à marginaliser le rôle de celle-ci comme “temple du savoir”. Les archivistes ont longtemps été les médiateurs indispensables pour accéder aux “Big data du passé”. La numérisation massive peut leur donner l’impression de se voir placer dans un rôle tout d’un coup plus périphérique. Pourtant, eux seuls connaissent intimement l’archive, eux seuls comprennent de manière précise la typologie des documents qu’elles contiennent. Leur rôle pour le projet est fondamental. Ils doivent devenir les experts de ces nouveaux projets de numérisation massive et participer activement à leur déroulement. 

*Note: C’est en impliquant dès le debut du projet Venice Time Machine, les archives d’Etat qu’il a été possible de convaincre les archivistes qu’en abordant la numérisation de cette manière, ils seraient extrêmement positifs qu’ils libèrent les images des documents numérisés en accès ouvert, permettant à chacun non seulement des les consulter, mais aussi de les télécharger pour leurs travaux*.

## Numériser massivement des documents anciens

Comment numériser efficacement des millions de documents anciens ? Il existe des machines semi-automatique utilisant des techniques robotiques pour maintenir les documents plats et garantir des images de très haute qualité sans risquer d’endommager les documents bien adaptées à ce genre de tâche. Ces machines n’hésite l’intervention d’un opérateur humain qui tourne les images de chaque registre et veille au bon déroulement de chaque étape de la numérisation. Cette personne doit travailler conjointement avec un archiviste qui réceptionne le document, note ces caractéristiques principales, effectuant en particulier la conversation entre la référence archivistique du document et les noms des images qui seront numérisé. L’archiviste peut aussi procéder à un reconditionnement du document avant ou après numérisation. 

Grâce à un travail d’analyse fait en collaboration avec les archivistes, la procédure peut être relativement standardisée en fonction des typologies de documents rencontrés. En développant un mobilier adéquat les déplacements et les gestes des deux opérateurs peuvent être rendus très efficaces et des performances s’approchant des 1000 pages numérisés par heure peuvent être atteinte. Pour augmenter la rapidité de la numérisation, il suffit alors de mettre en parallèle plusieurs unités de numérisation. Si l’archive elle-même ne permet pas d’accueillir ces unités, des architectures éphémères peuvent être installées dans les cours, les cloitres ou à proximité des bâtiments historiques qui sont souvent habitent les archives les plus anciennes. 

En parallèle de ces approches qui consistent essentiellement en une optimisation et une sérialisation des techniques traditionnelles de numérisation, d’autres recherches actuellement en cours laissent entrevoir des perceptives qui pourraient radicalement transformer la question de la numérisation massive. Pourrait-on par exemple numériser un registre ancien sans l’ouvrir ? Non seulement une telle technique permettrait d’aller beaucoup plus vite dans la numérisation, puisque tourner manuellement les pages reste aujourd’hui l’action qui prend le plus de temps dans le processus de numérisation mais elle permettrait également d’envisager la numérisation de documents dont la fragilité interdit l’ouverture. 

## Numériser les livres sans les ouvrir

Les techniques d’imageries médicales permettent depuis longtemps de regarder à très haute résolution, à l’intérieur des corps sans les ouvrir. La tomographie en particulier produit des modèles en trois dimensions à partir d’images pris selon une multiples d’angles. Ces images en trois dimensions peuvent ensuite être virtuellement traversées pour produire des “coupes” selon des angles choisis. 

La même approche pourrait a priori être développée pour les livres et les documents anciens. Beaucoup utilise en effet des encres à base de fer, opaque au rayon X. Les meilleurs résultats sont obtenus en utilisant la lumière très particulière produite à un synchrotron. Les premiers test effectués  sont extrêmement concluants. Le problème principal pour développer cette technique sous la forme d’un prototype fonctionnel pouvant éventuellement être installé au sein d’une archive est donc de produire des images avec une qualité équivalente à celle obtenu avec le synchrotron mais en utilisant une source d’énergie miniaturisable. Si ce problème pouvait être résolu, la numérisation massive pourrait devenir vingt fois plus efficace. 

Le développement d’un telle technique pourrait ainsi bouleverser en profondeur le domaine de de la numérisation massive. Il faudrait néanmoins adapter notre regard sur les documents “numérisés” par ce type de méthodes. Toutes les images produites ne seraient pas des photographies numériques mais des images recomposées virtuellement, coupe d’un modèle en trois dimensions, lui-même produit par une composition de milliers d’images. Plus que jamais, l’image qui s’affichera à l’écran sera fabriquée, résultat d’une chaine algorithmique complexe. Elle ne serait être comprise sans que ne soit développé en parallèle une nouvelle théorie de l’interprétation. 

*Note : Le professeur Giorgio Margaritando et la chercheuse Fauzia Albertin mène depuis XX des recherches dans ce domaine * …
*


## Transcrire des millions de documents

La numérisation n’est qu’une première étape. Il faut ensuite indexer le contenu des images numérisées, apprendre aux machines à les lire. Plusieurs années sont nécessaires à un philologue pour pouvoir déchiffrer la variété des écritures que l’on peut rencontrer dans les documents des archives vénitiennes. Comment une machine pourrait-elle y arriver ?

Par rapport à un tel défi, l’énorme quantité de documents que nous devons transcrire n’est pas un problème, c’est au contraire une partie de la solution. La grande quantité des documents nous permet en effet de repérer des motifs graphiques récurrents (mots, abréviation, lettrine, etc.) Un algorithme segmente les documents en repérant ces motifs, puis calcule une distance graphique entre chaque motifs. Le même mot écrit par le même scripteur a une apparence similaire que l’algorithme arrive évaluer. La distance augmente quand on change de scripteur ou de mots. Par ce procédé la masse documentaire devient un réseau de motifs graphiques reliés les uns avec les autres, avec pour chaque lien une estimation de la probabilité qu’il s’agisse du même motif. Ainsi si nous donnons la possibilité à un lecteur de transcrire un mot, proposant ainsi une correspondance directe entre une forme graphique et une séquence de lettre, une série d’inférence permet ensuite de déduire des transcriptions possibles pour toute une série d’autres motifs présents sur les documents. Chaque transcription est associée à un degré de confiance ou d’incertitude.

À cette logique de correspondances graphiques se superposent les connaissances sur la langue et la structure des documents dont nous pouvons disposer pour une époque particulière sous la forme de corpus de textes déjà transcrits. Les philologues et les historiens travaillent depuis longtemps pour établir des éditions critiques des textes et séries documentaires les plus importantes des archives. Nous pouvons extraire de ces corpus de textes datant d’époques différentes, des modèles statistiques. Plus précisément, nous pouvons construire, à partir de grande quantité de textes transcrits dont nous disposons, un modèle calculant les probabilités de transitions entre les mots pour une année spécifique du passé. Ce modèle peut-être utilisé pour améliorer les inférences lors du processus de transcription semi-automatique des documents. Si graphiquement deux transcriptions peuvent correspondre à un même motif visuel, une d’entre elles est peut-être beaucoup plus probable si l’on se base sur les statistiques des textes de l’époque correspond.

Ce jeu d’inférences statistiques transforme en profondeur les méthodes de transcription. L’objectif n’est plus forcement d’obtenir une transcription parfaite, fruit d’un travail parfois de plusieurs années, mais d’améliorer constamment la qualité des transcriptions partiellement automatisées en transcrivant spécifiquement certains documents ou passage. Les algorithmes eux-mêmes peuvent donner des indications sur les passages qu’il serait le plus utile de transcrire pour améliorer globalement la qualité de la transcription générale.

## Construire le graphe sémantique

À partir des transcriptions complète ou partielles, il possible de repérer ce que nous appelons les “entités nommées”. Il s’agit typiquement des personnes, des lieux et des institutions. Un double processus permet de le faire. Nous pouvons d’une part repérer certains noms à partir de liste de lieux ou de personnes déjà établis et d’autre part inférer selon la syntaxe la présence d’entités nommées nouvelles. Dans un contrat de travail ou un testament par exemple, le nom de l’apprenti ou du défunt est en général introduit à un moment particulier. Le repérage le plus efficace de ces éléments variables du texte se fait durant le processus de transcription, où un lecteur peut spécifier qu’un segment graphique particulier correspond à un nom ou un lieu. Le système d’inférence peut ensuite tenter de prédire, selon la structure des documents quand ces éléments sont susceptibles d’être présents.

Les entités nommées participent à ce que nous appelons des entités temporelles, typiquement des évènements. Pour encoder des informations historiques, il est en général préférable de modéliser les changements plutôt que les états. Par exemple plutôt que noter dans une base de données la date de naissance d’une personne, il est plus astucieux de créer une entité temporelle correspondant à l’évènement de sa naissance. Nous pouvons en fait dans ce cas plus facilement associer à cette entité un lieu, la présence d’autres personnes, etc.

Les entités nommées et les entités temporelles constituent les sommets d’un graphe immense dont les arêtes précisent les relations particulières que ces entités entretiennent les unes avec les autres. La structure de ce graphe a des similarités avec celle qui code l’information dans les réseaux sociaux. D’une certaine manière, nous essayons de construire ici, un “Facebook” du passé. Dans ce graphe nous pouvons par exemple facilement indiquer qu’une personne X a participé à un évènement Y ayant lieu dans un lieu Z durant un intervalle temporel T. 

Ce codage sous forme de graphe nous permet plusieurs types d’inférence. Si deux personnes ont participé à un même évènement, cela implique qu’elles ont été ensemble dans le même espace pendant le même intervalle temporel. Cela suppose également que ces personnes doivent être nées avant cet évènement et pas encore mortes. Ces contraintes qui semblent évidentes lorsque l’on code une information historique prennent toutes leurs intérêts quand des millions d’informations de ce type sont extraites de grandes séries de documents. Chaque nouvelle information doit pouvoir s’inscrire dans l’espace délimité par les autres informations pour former un ensemble cohérent. Si des incohérences apparaissent, il devient nécessaire de considérer qu’il existe différentes reconstructions possibles du passé, incompatibles les unes avec les autres.

Le codage des lieux pose des problèmes spécifiques. La notion de lieu est en effet un concept relativement complexe. Coder un lieu par ces coordonnées GPS, une idée qui semble naturelle aujourd’hui, se révèle être une stratégie très mal adaptée dans un contexte historique. Un pays comme la France ou une ville comme Athènes peuvent être considérés comme des lieux correspondant à une géométrie spatiale relativement stable. Mais il existe d’autres types de lieu qui bougent dans le temps. Certains évènements peuvent avoir lieu sur bateau. D’autres dans un bar dont l’adresse pourra changer plusieurs fois, mais qui conservera une même identité dans le temps. La notion de lieu n’est donc pas a priori une notion géométrique. Les lieux se définissent toujours relativement à d’autres lieux plus larges et sont toujours potentiellement en mouvement par rapport à ces référentiels. Ce type de précaution est crucial lorsque l’on conçoit les systèmes d’information qui doivent accueillir les données historiques.

## Encoder les informations métahistoriques

Comment passer des documents transcrits à ce type de codage sémantique ? Le travail peut être fait manuellement par des historiens qui interprètent les documents et encode les informations dans une base de données. Dans certains cas, le processus peut être en partie automatisé lorsque les documents suivent une logique très régulière, par exemple pour certains documents administratifs comme des contrats qui spécifient toujours de la même manière les relations qui lient un certain nombre d’acteurs. Un contrat d’apprentissage vénitien comportera ainsi le nom de l’apprenti, le nom du maître, l’adresse de son atelier, le salaire qui sera versé.

Quelle que soit la méthode, il est extrêmement important que les processus techniques et historiques qui conduisent à construction de l’information soient documentés avec précision. En effet, ces informations métahistoriques sont nécessaires pour comprendre la nature de l’information encodée dans le graphe sémantique. Elles peuvent expliquer des éventuelles incohérences dans les données encodées. Elles permettent d’envisager avec précaution la fusion de jeux de données venant de groupes de recherche différents, utilisant des méthodes variées. Elles ouvrent la voie à la reconstruction de passé possible en ne considérant que certains types de documents ou de méthodes.

Les informations métahistoriques peuvent se documenter sous la forme d’un graphe similaire à celui qui sert à coder l’information historique. Des acteurs (les chercheurs) participent à des événements (une transcription, la vectorisation d’une carte) qui ont lieu dans l’espace et le temps et conduisent à la production de certains objets (cartes, images, transcription, etc.). Selon les interfaces utilisées, elles peuvent être documentées automatiquement sans faire perdre du temps au chercheur.

Ainsi chaque étape du processus de construction de l’information historique peut être documentée. Il devient possible d’expliciter le processus de sélection des sources, les phases de transcriptions et les différents processus interprétatifs, qu’ils soient réalisés par des humains ou des machines. Cette approche ouvre la voie à des systèmes informatiques capables de gérer de situations dans lesquelles il n’y a pas qu’une seule vérité historique, mais diverses reconstructions possibles, correspondant à des espaces de connaissances particuliers, tous parfaitement documentés.

## Le déploiement spatiotemporel

(A reexaminer)

Le graphe sémantique peut ensuite être déployé dans le temps et l’espace, mais pour cela il faut reconstruire des cartes du passé. Il existe pour une ville comme Venise de très nombreuses cartes détaillant avec précision les évolutions urbaines. Les relevés cadastraux des 200 dernières années peuvent être facilement adaptés pour intégrer un système d’information géographique historique. Chaque relevé cadastral donne une “photographie” à une date précise de la structure urbaine de la ville. Une des difficultés est de compléter l’espace temporel qui sépare deux de ces “instantanés”. Un bâtiment peut, par exemple, être présent dans le cadastre napoléonien du 1805, mais pas sur le cadastre autrichien, environ trente ans plus tard. En l’absence d’autres informations, il faut faire une inférence probabiliste si nous souhaitons représenter la situation dans une année intermédiaire. Plus nous serons proches de la date du cadastre napoléonien, plus il y aura de chance que le bâtiment soit encore présent. Dans tous les cas, il nous faut à nouveau accepter l’idée qu’il n’y a pas une seule bonne carte, mais plusieurs plus ou moins probables.

Pour exploiter les cartes du XVIIIe, XVIIe et du XVIe siècle nous devons procéder différemment. En effet la plupart de ces cartes offrent des vues en perspective de la ville, très détaillées et riches sur le plan architectural, mais pas directement superposables aux cartes actuelles. L’information sur ces cartes est avant tout topologique. Nous pouvons déduire quel palazzo jouxte quel campanile. Nous pouvons croiser cette information avec d’autres sources, comme les déclarations d’impôts qui elles aussi donnent des informations topologiques, comme la description de la position d’une boutique par rapport à la proximité d’un palais. En combinant ces sources, un jeu de contraintes se met en place, permettant d’élimer certaines solutions, mais laissant de la place à diverses configurations. Il s’agit en quelque sorte de résoudre un Sudoku géant en partant de certaines hypothèses et en évaluant si elles sont compatibles avec le reste des données disponibles.

Faute de cartes détaillées, nous devons pour reconstruire Venise avant 1500 en nous basant seulement sur des données archéologiques et des témoignages indirects. L’exercice est plus périlleux, la place des hypothèses de travail étant extrêmement importante. Néanmoins, dans la mesure où ces choix sont parfaitement documentés, il est envisageable de simuler les configurations urbaines qui font décrivent l’évolution progressive de la ville depuis les premiers ilots “terraformés” jusqu’aux cartes détaillées de 1500. Pour reconstruire ainsi la « morphogenèse » de la ville, l’historien doit extrapoler à partir des données existantes, extraire les régularités — des grammaires —, qui gouvernent les formes connues pour simuler leur application dans un nouveau cas non directement documenté. Ces grammaires ont certaines similarités avec celles que nous mentionnions pour la transcription des textes, mais elles sont ici utilisées non pas pour aider à la reconnaissance, mais pour générer des configurations urbaines possibles.

## Les approches procédurales

En informatique, ce type d’approche appartient à la grande famille des techniques d’extrapolation, et plus particulièrement à ce qu’on appelle les approches génératives ou procédurales. Une grammaire générative permet par exemple de tracer un réseau de rues ou de visualiser des façades de bâtiments qui n’existent plus. Ces «architectures» procédurales peuvent cohabiter avec des modélisations plus traditionnelles comme dans le cas du projet Rome Reborn, une reconstruction de la Rome antique en 320 AD. Le modèle de la ville utilisé dans ce projet combine aussi bien des éléments de classe I, dont la position, l’identification et l’apparence sont connues avec une assez grande précision, et des éléments de classe II pour lesquels nous ne disposons que d’informations indirectes et imprécises. Seuls 250 bâtiments parmi les 7000 bâtiments que compte la reconstitution appartiennent à la première catégorie. Tous les autres sont construits selon des hypothèses architecturales et historiques formalisées dans un algorithme.

*Note : REF Rome Reborn*

Les approches procédurales qui encodent les hypothèses historiques dans des algorithmes ne se limitent aucunement au domaine des reconstitutions architecturales. Pour s’en convaincre, prenons une autre famille d’exemples, cette fois-ci centrés sur la modélisation des routes maritimes historiques. L’étude des arts nautiques, les traités de navigation, l’étude des navires et bien sûr les recherches sur la circulation des biens ou des informations ont donné lieu à un grand nombre de monographies. En appliquant ces recherches aux routes maritimes vénitiennes et en les associant aux très riches informations disponibles dans les Archives d’État, une reconstitution fine des échanges en Mer Méditerranée est envisageable.

Certaines routes sont particulièrement bien documentées, notamment le système des galées du marché voyageant chaque année en convoi entre la fin du 13e siècle et le milieu du 15e siècle. Sur ces trajets, il est possible à partir des données disponibles de tracer les itinéraires de chaque convoi, année après année. À partir de l’encodage de ces informations, nous pouvons reconstruire un véritable simulateur de trajets qui nous permet de répondre à des questions comme : En juin 1322, quel est le prochain bateau de Constantinople à Corfou ? Combien de temps prendra le voyage ? Quelles sont les chances de rencontrer des pirates ?
Mais malgré leurs détails, les données des archives ne descendent pas en dessous d’une certaine précision dans la description des itinéraires. Par exemple, seules les escales commerciales obligatoires sont notées. Or il est extrêmement probable que les navires fassent des escales intermédiaires notamment en Mer Adriatique ou lors de recherches d’informations (présence de dangers, piraterie…). Si l’on souhaite aller à ce niveau de détails pour simuler, mois par mois, la position probable des navires, plusieurs hypothèses peuvent être proposées.

Ces hypothèses se basent sur les connaissances recueillies sur la navigation de l’époque, mais, comme dans tout processus de recherche académique, elles peuvent différer les unes avec les autres, notamment dans la part relative qu’elles accordent au cabotage ou à la navigation en haute mer. Ici, comme en architecture, il peut y avoir débat. Et dans le nouveau régime visuel des cartes géohistoriques, ce débat prend la forme d’algorithmes contradictoires qui encodent des trajets différents.

Ces exemples nous illustrent comment la cartographie en étendant le domaine de représentation de l’histoire effectue en même temps un déplacement du débat scientifique. Qu’il s’agisse de tracés de rue, de réseaux d’information, de modélisation architecturale, de flux commerciaux ou de trajets maritimes, les chercheurs ne peuvent plus se contenter de débattre linguistiquement avec des argumentaires et des raisonnements, mais en opposant dans le domaine cartographique leurs hypothèses appareillées sous la forme d’algorithmes.

## Une éthique de la représentation

La force des systèmes d’information historique est d’uniformiser les données produites par des méthodes différentes dans un seul modèle informationnel capable de reconstruire à la volée des cartes plausibles pour n’importe quelle date des 1000 dernières années. Il faut néanmoins prendre des précautions quant à la manière de représenter ces cartes hypothétiques.

La représentation sous forme de cartes, en faisant basculer une représentation essentiellement textuelle et narrative en une représentation visuelle et exploratoire, introduit un mouvement visant à expliciter chaque étape des processus de constitution des données historiques sous-jacentes aux représentations. Pour ne pas tromper celui qui lit les représentations visuelles et synthétiques d’un passé reconstitué, il faut développer des outils et des processus garantissant une « éthique de la représentation ». Ces développements impliquent la constitution de nouvelles normes visuelles.

Les lecteurs de cartes attendent peut-être implicitement un niveau de précision éventuellement incompatible avec le niveau de certitude que les données peuvent raisonnablement fournir. Alors qu’un nombre toujours plus grand de projets s’attelle à reconstruire des cartes du passé, la question de la représentation de l’incertitude a pris à grande importance dans la communauté des chercheurs qui travaillent sur ces questions. Nous avons discuté plus haut qu’une manière d’approcher ce problème est d’associer à chaque processus intellectuel et technique qui permet la construction des cartes ou d’autres formes de représentation visuelle à des données documentant la manière dont sont constituées les données historiques représentées. D’un point de vue éthique, plus les reconstitutions du passé sont susceptibles d’impressionner, plus rigoureux et transparent devrait être le processus intellectuel et technique sous-jacent.

Différentes méthodes ont été proposées pour visualiser l’incertitude de données. Il est possible d’utiliser par exemple différents niveaux d’opacité pour signifier le degré d’incertitude présente sur les cartes historiques. La même approche peut être utilisé dans le cadre de représentations architecturales urbaines (opacité très dense quand le bâtiment est encore présent, dense quand le bâtiment existe encore, mais a subi des rénovations importantes, moyen quand il n’existe plus qu’une ruine, très peu dense quand la seule source de présence de ce bâtiment est indirecte, indiqué par exemple par une position sur un plan). Ces informations permettent de qualifier la nature de la reconstitution 3D résultante. Elles sont une première indication qui peut être complétée par un accès aux données métahistoriques documentant la chaine de processus sous-jacents aux données représentées.


## La rencontre entre des dizaines de milliers d’étudiants et des millions de documents

L’ensemble des facteurs que nous venons d’analyser nous laisse à penser que les sciences humaines sont sur le point de vivre un bouleversement comparable à celui qui a frappé la biologie dans les trente dernières années. Cette révolution consiste essentiellement en un changement d’échelle dans l’ambition et la taille des projets de recherche. Il nous faut former une nouvelle génération d’étudiants capables de maîtriser les algorithmes comme outils de connaissance et de débats, mais aussi de s’interroger sur ces changements. En rapprochant recherche et éducation, cette nouvelle génération d’étudiants pourrait non seulement bénéficier de ces projets de grande ampleur, mais jouer un rôle crucial dans leur succès ou leur échec. Il s’agit “simplement” d’organiser, chaque année, la rencontre entre des dizaines de milliers d’étudiants et des millions de documents.

Pour tenter d’illustrer comment cette convergence entre éducation et recherche pourrait avoir lieu, prenons un exemple. Nous avons décrit plus haut les techniques générales qui permettent d’envisager la transcription semi-automatique de documents anciens. Nous sommes en train de préparer un cours en ligne sur ce sujet particulier. Le cours présente à la fois les bases en terme de philologie pour s’initier à la lecture des écritures et l’interprétation des documents anciens et introduit les techniques générales pour automatiser en partie leur transcription. Comme dans tous processus partiellement automatisés, ces techniques gagnent beaucoup en efficacité quand elles sont adaptées aux caractéristiques spécifiques de chaque famille de document. Si l’algorithme peut bénéficier d’information sur la langue, la structure des documents, les caractéristiques des écritures, ses performances seront décuplées. Or appliquer des connaissances générales apprises en cours à des cas spécifiques est la base d’une démarche pédagogique efficace. C’est ce que souhaitons proposer aux étudiants de ce cours.
Depuis quelques années les cours en ligne massifs (MOOCs) ont gagné en popularité. Certains cours de l’École Polytechnique Fédérale de Lausanne sont suivis, chaque semestre par des dizaines de milliers d’étudiants dans le monde entier. Ces cours sont exigeants et demandent un travail régulier de la part des étudiants. Sur 50 000 étudiants inscrits la première semaine, seulement 20 % arrivent en général au bout du cours. Mais, même avec un taux d’abandon à 80 %, cela représente encore 10 000 étudiants motivés.

Imaginons pour simplifier que les documents de notre archive sur lesquels nous souhaitons que les étudiants travaillent s’étalent sur un millier d’années. Découpons cet immense corpus en décade regroupant des documents ayant une certaine homogénéité formelle et linguistique. Nous pouvons former ainsi 100 groupes de documents homogènes. Avec 10 000 étudiants nous pouvons encore associer à chaque 100 étudiants à chaque catégorie de document. Faisons les travailler en groupe de 5, constitué de profils complémentaires (historiens, informaticiens, linguistes, etc.) et demandons à chacun des 20 groupes ainsi constitués d’adapter les principes généraux vu en cours aux spécifiés de la famille de documents dont ils ont la charge. Tous les étudiants apprendront par une telle démarche et il est très probable que, sur les 20 groupes, au moins un obtienne un résultat de très bonne qualité. En un semestre, ce sont 100 nouveaux algorithmes optimisés pour la transcription automatique de familles de documents homogènes qui sont disponibles. Les étudiants ont appris, le projet a fait un bon avant, la procédure peut être réitérée au semestre suivant.

## Comment le travail de chacun s’enrichit du travail de tous

Il est essentiel de comprendre que ce changement d’échelle dans les sciences humaines n’exclue pas les modes de recherche plus traditionnels. Au contraire, les grandes bases de données multidimensionnelles ainsi constituées offrent un environnement qui facilite le travail sur des sujets spécifiques.
Si j’étudie un peintre du XVIe siècle, je pourrais profiter du travail d’extraction que d’autres chercheurs ont effectué sur les contrats d’apprentissage documentant ainsi l’ensemble des ateliers de la ville et tous les apprentis (déclarés) qui y travaillaient. Je voudrais peut-être savoir les couleurs disponibles à cette époque et trouverait l’information dans le modèle des échanges maritime construit par un autre groupe de recherche à partir des archives. En me basant sur les cartes reconstituées montrant le tissu urbain et simulant certaines des façades que ce peintre a peut-être peintes, je pourrais contredire ou compléter les informations présentées dans la base géohistorique urbaine du projet. Mon travail en retour viendra densifier la base de données grâce aux informations biographiques et iconographiques nouvelles qu’il a contribué à structurer.

Dans le même ordre d’idées, la grande base de données multidimensionnelle pourrait me permettre de mieux comprendre un moment particulier dans l’évolution de la musique vénitienne au tournant de l’âge baroque. Comment par exemple expliquer le grand bouleversement qui, avec l’arrivée de la musique baroque, conduit les partitions musicales à redevenir manuscrites, phénomène peut-être unique dans l’histoire des médias. Au XVe et XVIe siècle, Venise dominait le marché des partitions imprimées. Mais les changements musicaux introduits par le baroque, à l’époque de Monteverdi, ont posé un problème à l’industrie typographique. La nouvelle musique ne pouvait plus s’imprimer sans inventer de nouveaux systèmes techniques. Cette crise se conjugue avec un déclin économique, car cette transformation apparaît au moment où la cité des doges perd du terrain en Méditerranée face aux Ottomans. Enfin la nouvelle musique cesse d’être une musique populaire pour au contraire conquérir les élites. Dans ces relations imbriquées, quelles sont les causes et quelles sont les conséquences ? Comprendre comment la musique redevient manuscrite nécessite de combiner les points sur vues artistiques, techniques, sociologiques et économiques.
Le défi de la Venice Time Machine est d’amener toutes ces recherches à s’articuler les unes par rapport aux autres, de façon à livrer une vision globale de la ville et de ses réseaux à travers 1000 ans d’histoire. Le défi n’est pas que technique. Il faut que les différents chercheurs qui contribuent cet environnement de recherche sentent que ce projet est aussi le leu, un bien commun collectivement construit.

## Un modèle multidimensionnel de Venise comme prototype pour un programme européen plus vaste.

L’histoire de Venise, celle d’un modeste village de pêcheurs protégé par une lagune qui devient en quelques siècles le plus grands empire maritime de la Méditerranée, est fascinante à reconstituer. Comme nous l’avons vu, nous disposons de données innombrables pour reconstruire cette histoire, car le développement de Venise s’est très vite accompagné de la mise en place d’un système bureaucratique dans lequel tout était documenté. L’enjeu est de constituer une base ouverte multidimensionnelle et collaborative, en perpétuelle densification.

La première dimension de cette base est environnementale : l’évolution de la lagune, sujet de grande attention aujourd’hui, a toujours joué un rôle crucial dans l’histoire de Venise. La ville s’est construite en coévolution avec elle. La seconde évolution est urbaine et architecturale. Nous possédons des informations assez précises qui permettent de reconstruire la « morphogenèse » de la ville, construite d’ilot en ilot au fil des siècles. La troisième dimension regroupe les activités des hommes. Grâce aux très riches archives de la ville, nous pouvons reconstituer des informations démographiques assez détaillées. Nous pouvons également modéliser les circulations et les échanges, au sein de la lagune puis avec le développement de l’empire maritime sur une grande partie de la Méditerranée. Enfin, la dernière dimension concerne les productions humaines, linguistiques, culturelles et artistiques, résultant de toutes les influences issues du fantastique réseau socio-économique de Venise en Europe. Comme nous l’avons vu, toutes ces dimensions s’influencent les unes et les autres.

Venise est un exemple emblématique, mais la même approche pourrait être déployée sur d’autres archives et d’autres villes. Les techniques et les méthodes développée sur la cité des doges pourrait être appliquée pour construire d’autres machines à remonter le temps pour des villes comme New York ou Bagdad.  Il ne fait pas de doute que L'Europe est la mieux placée mondialement pour être en pointe dans ces nouveaux champs de recherche: il y a ici une histoire millénaire, riche et complexe, les meilleures spécialistes de ce passé et le multilinguisme comme culture profonde. Mais l’approche peut être généralisée. J ’imagine d’ici une vingtaine d’années, la construction collective d’une machine à remonter le temps globale couvrant la surface entière de globe et permettant de négocier une histoire commune, à partir de centaines de passés possibles. Pour que ce rêve devienne réalité il faudra non seulement relever certains défis techniques mais aussi former une nouvelle génération de chercheurs, à la frontière entre les sciences humaines et les sciences de l’information, capable de produire, organiser et mettre en scène les “Big data” du passé. 






#  Comment lire 20 millions de livres ?

En Mars 2012, Google a annoncé avoir numérisé 20 millions de livres. Le chiffre donne le vertige mais ne se laisse pas facilement appréhender. Que représentent 20 millions de livres ? Procédons d’abord de manière comparative. 20 millions de livres numérisés est une quantité significativement plus importante que la plupart des autres programmes de numérisation. Le nombre d’ouvrages numérisés dans l’initiative européenne Gallica est environ 1 million , Hathi Trust annonce “seulement” 6 millions de livres numérisés, Internet Archive près de 3 millions. 20 millions ne représentent évidemment qu’une portion de tous les livres publiés. A titre comparatif, la librairie de Congrès américains posséderait 32 millions d’ouvrages et le catalogue de l’OCLC récence près de 170 millions de livres physiques dans les bibliothèques du monde. Il est évidemment difficile d’estimer l’ordre de grandeur du nombre total de livres publiés dans l’ensemble des langues du monde, mais il s’approche sans doute autour de centaines de millions d’écrits.

Essayons maintenant de nous représenter physiquement cette numérisation. Une estimation nous est donnée par les mesures que la Michigan Library, un des premiers partenaires de Google, a effectué après avoir complété la numérisation du premier million de livres. Ce premier million de livres représentait 25.8 km linéaires, 680 tonnes, 361 millions de pages, 70 000 millions de mots écrits dans près de 430 langues différentes. Paradoxalement, cette immense quantité d’information tenait dans “seulement” 42 Terabytes de mémoires. Multiplions ces chiffres par 20 et nous obtenons un estimation des ordres de grandeur du projet Google Book. Si ce raisonnement est correct, 20 millions de livres correspondrait à 516 km de documents et 13 600 tonnes de livres déplacés, 7220 millions de pages, 1 400 milliards de mots et “seulement” 140 Terabytes de données informatique.  

Nous sommes donc face à un objet numérique immense, long de plusieurs centaines de kilometres s’il devait être imprimé, un mégatexte qui continue de croitre en taille et en densité.  Ce chapitre est consacré à l’étude de ce “mégatexte” et à la manière dont il interroge nos pratiques culturelles. 

Le corpus de Google books questionne notre culture par sa simple existence. Qui aurait pu prédire qu’en seulement quelques années, un tel projet a pu être réalisé par une simple entreprise de la Silicon Valley ?  Il est utile de reconstruire chronologiquement les principales étapes de cette aventure pour tenter d’en comprendre la logique opérationnelle. La manière dont Larry Page a réussi à convaincre un à un chaque partenaire du projet est porteuse de leçons pour le futur. Google a certes bénéficié de son grand pouvoir financier pour se lancer dans cette aventure mais il a su aussi convaincre un certain nombre d’acteurs très pragmatiques qu’il était possible de réaliser en quelques années un numérisation de masse. Si nous détaillons dans ce chapitre l’aventure de Google, ce n’est ni pour faire l’apologie ni même la critique de l’entreprise américaine mais pour, à partir de cette exemple historique, tirer des leçons sur les processus effectifs qui sont à la génèse de ces nouveaux objets du XXIe siècle : les “mégatextes”. 

Ce corpus pose aussi une question économique, que nous discuterons brièvement.  Dans l’interface que Google propose, nous ne pouvons voir qu’une petite partie de cet océan documentaire. En effet, la plus grande partie des livres numérisés sont encore sous copyright et ne peuvent être affichés sans enfreindre la loi. Il est estimé que seulement 20% des livres numérisés par Google sont libres de droit. Le coeur de l’archive de Google est constitué par livres encore exploités commercialement. C’est la masse caché du trésor accumulé par l’entreprise américaine. Depuis quelques années, Google a entamé une longue négociation pour trouver un accord commercial pour l’exploitation de ces livres. Il ne fait pas de doute que cette négociation est sans aucun doute une des plus importantes de notre époque car son issu structurera le paysage de la culture pour de nombreuses années. Les enjeux économiques ne doivent cependant pas occulter une question plus complexe sur la nature même de ce trésor caché. De quoi s’agit-il véritablement d’une collection d’images, d’une collection de textes, d’un système de connaissances ? 

Ce corpus bouleverse enfin les pratiques académiques. Son immensité pose aux chercheurs en sciences humaines un défi paradoxal. Il invite à questionner les modes de recherche basé sur la lecture attentive d’un tout petit nombre de texte en posant potentiellement le problème de leur représentativité. En donnant à voir la masse documentaire, il remet en question l’importance de l’attention portée sur l’oeuvre de certains auteurs, peut-être exceptionnel mais pas forcement représentatifs ni de ce qui était écrit ni de ce qui était lu à une époque donnée. Ce questionnement s’accompagne d’un malaise méthodologique. Comment étudier des millions de livres sachant qu’il sera impossible de lire chacun d’eux ? Quel crédit donner à des études qui se contentent d’analyser  la surface textuelle des oeuvres, sans en comprendre le sens ? Peut-on véritablement cartographier des millions de livres sans les lire ? Il convient donc de s’interroger sur les forces et les faiblesses de ces nouveaux modes d’analyse littéraire, et de montrer en discutant des exemples précis comment il devient possible de caractériser globalement la production littéraire d’une époque ou à l’inverse de discuter de la structure d’une œuvre particulière aidée par des nouveaux programmes de lecture. Le mégatexte, cet objet à une autre échelle, produit par un processus industriel et algorithmique ne peut lui-même être appréhendé que par la médiation des algorithmes. 

##  L’analogie web-bibliothèque

Le projet Google Books est latent dans l’histoire de Google avant même que l’enterprise ne soit créée. En 1996, étudiants a Stanford, Larry Page  travaille comme projet de recherche sur le *Stanford Digital Library Technology Project* et réfléchit à des technologies qui permettrait de naviguer différemment dans le contenu de grandes bibliothèques numériques.  C’est dans ce contexte, qu’il imagine l’idée de “crawlers” capable d’analyser automatiquement le contenu de livres dans le but de classer leur pertinence par rapport à une requête donnée. Son idée est simplement de se baser sur le principe de la citation académique pour évaluer la pertinence d’un livre donné. Il propose de calculer la pertinence d’un livre à partir à la fois de quantité d’autres livres qui le cite et la pertinence de ces livres.  Chaque citation d’un livre est comme un vote dont le poids est proportionnel à l’importance du livre qui cite, importance elle-même basée sur le nombre de citations dont ce document bénéficie. La méthode de calcul est donc récursive. Le premier algorithme conçu par Page sur ce principe s’appelle “BackRub”. Il est l’ancêtre de ce qui va faire le succès planétaire de Google, l’algorithme “PageRank”. 

En 1998, la plupart des moteurs de recherche permettaient de sélectionner un sous-ensemble de pages web contenant une série de mots clés particuliers, mais l’ordre dans lequel ces pages apparaissaient était établi à partir de méthodes peu fiables, comme le nombre d’occurrences d’un mot clé particulier dans le contenu d’une page. Ces méthodes donnaient des résultats de moins en moins bons au fur et à mesure que le web grandissait. Page fait alors une analogie heureuse : D’une certaine manière, le web n’est qu’une grande bibliothèque et la méthode de calcul recursive inventé pour le projet de bibliothèque numérique de Stanford peut s’appliquer d’une façon plus générale au web dans son ensemble. Sergei Brin et Larry Page créent l’enterprise Google sur la base cette intuition.

*Note : Clarifier le rôle de Brin*

Contrairement aux techniques de classements des autres moteurs de recherche, avec ce principe de vote, la qualité des résultats de recherche s’améliore en principe au fur et à mesure que le Web s’étend. Plus il y a de documents, plus il y a de citations et plus la classification est précise. Les internautes découvrirent rapidement que celle nouvelle méthode donne effectivement de meilleurs résultats et Google dépasse progressivement en popularité les autres moteurs de recherche de cette époque. Dès l’année 2000, il est devenu la porte d’entrée la plus populaire pour accéder au Web. 

##  La transformation d’une utopie en une réalité 

L’analogie web-bibliothèque fonctionne dans les deux sens. Si le web est comme une bibliothèque, le web peut aussi accueillir la plus grande bibliothèque numérique du monde. En 2002, 4 ans après la création de l’entreprise, un petite équipe chez Google commence à réfléchir au projet de numériser massivement une grande quantité de livre et de proposer ces images sur le web. Cette équipe n’est constituée de professionnels de la numérisation, suggérant que pour s’attaquer à une aventure comme la numérisation de plusieurs millions de livres pour constituer le plus grand corpus de textes numérique jamais établi, il ne suffit que d’un bon sens pratique. Pour se rendre compte de la difficulté d’une numérisation de masse, Larry Page et Marissa Mayer font une expérience simple. Il chronomètre le temps qu’ils leur faut pour numériser manuellement un livre, en tournant les pages une par une. Avec l’aide d’un metronome, pour garder un rythme constant, ils évaluent ainsi qu’en 40 minutes, ils peuvent venir à bout d’un livre de 300 pages. Il suffit de multiplier ce chiffre par plusieurs millions pour avoir une idée grossière du temps et de l’argent nécessaire pour mener à bien ce projet. 

*Note : Biographie de Marissa Mayer*

C’est avec cet ordre de grandeur en tête, qu’ils entament un tour aux Etats-Unis pour faire l’état des lieu des projets de numérisation les plus ambitieux, comme le projet Gutenberg ou le Million Book project. En parallèle, Page decide d’approcher les cinq bibliothèques les plus importantes des Etats-Unis, mais en utilisant d’abord les relations personnelles qu’il a construit dans son parcours universitaire. 

C’est ainsi que Page commence par la bibliothèque de Michigan, là où il a fait son Bachelor quelques années plus tôt. Cette bibliothèque est en fait constituée par un système de 19 bibliothèques spécialisées, hébergeant en 2008 un total de près de 10 millions de livres et faisant l’acquisition de près de 180 000 nouveaux livres par an. Ayant participé au programme Jstor pour la numérisation de milliers de journaux académique, les dirigeant sde Michigan ont une certaine familiarité avec les programmes de numérisation massif. Page rencontre Mary Sue Coleman,  présidente de la bibliothèque, pour lui exposer ses plans. La direction de la bibliothèque estime qu’au rythme actuel il faudrait environ 1000 ans pour numériser 7 millions de livres. Page annonce avec sérieux qu’il peut faire la même chose en moins de 6 ans. 

Page se rend ensuite à Stanford, là ou l’aventure Google a commencé. Il retrouve son mentor, le professeur Terry Winograd, un des grands pionniers de l’intelligence artificielle et Paul Allen, le co-fondateur de Microsoft. Il est les convint de la faisabilité de son projet puis invite à Michael Keller, le directeur de la bibliothèque de Stanford de venir au *GooglePlex* , le site principal du l’entreprise, en 2003  et lui propose de numériser 9 millions de livres pour les mettre en ligne. 

Page est maintenant prêt pour aller s’attaquer à un partenaire plus difficile à convaincre : La bibliothèque de l’université d’Harvard. Bien plus ancienne car fondée en 1638, elle regroupe près de 90 bibliothèques spécialisées. La bibliothèque Widener héberge par exemple à elle-seule 16 millions de livres et la bibliothèque de droit donne accès à 2 millions de volumes. Page rencontre Sydney Verba, qui reste très sceptique sur la faisabilité technique du projet. Page lui propose de venir en Californie pour voir les machines à numériser ultra sophistiquée qu’il pense utiliser pour le projet. Verba fait le voyage et est très impressionné par la technologie californienne. Il décide d’accepter de travailler avec Google. L’ironie est que, quand le projet débute, Page choisit pragmatiquement de ne pas utiliser de machines automatisées mais préfère opter pour une armée de tourneurs de pages humains, solution plus économique et plus flexible. La démonstration technologique hollywoodienne n’aura été utile que pour impressionner les acteurs conservateurs de la cote est. En 2007, Robert Darton qui remplace Verba et se montrera beaucoup plus critique sur le projet Google. 

La prochaine étape est Oxford et la Bodleain library, fondée en 1602 et hébergeant parmi les collections de livres anciens les plus précieuses du monde. La chance joue un faveur de Google. Reg Carr cherche précisément des fonds pour numériser une partie de ses collections.  Il rencontre Raymond Nasr, chef de la communication de Google et conclue rapidement un accord pour numériser un million de livre du XIXe siècle qui sont dans le domaine public. 

La dernière bibliothèque que Google veut convaincre est la New York Public Library qui dans son bâtiment de la 5e avenue héberge près de 16 millions de documents. Le directeur Paul Leclerc accepte de travailler avec Google pour numériser les livres du domaine public et ceux qui ne présentent pas de fragilité spécifique. 

Pas à pas, rencontre après rencontre, Larry Page a réussi l’impossible : convaincre cinq immenses bibliothèques à participer à un projet que tous s’accorde à penser comme impossible. Mais le plus dur commence, il va maintenant falloir effectivement réaliser le travail. 

Le projet commence en 2004, sous le nom de code  Océan. Quelques informations filtrent dans la presse mais sans avoir un retentissement véritable. Certains pensent que le projet est lié à Google Earth. En Octobre, Google loue un stand à la Buchmess de Franckfort pour y présenter le projet qui s’appelle maintenant “Google Prints for Editors”. Page et Brin font le déplacement et dévoilent, sans rentrer dans les details, l’existence d’un projet bien plus ambitieux appelé “Google Print”. Personne ne les prend au sérieux. La Frankfurter Allgemeine titre ironiquement “Les fondateurs de Google ont trouvé un intérêt pour les livres”. 

Puis le 14 décembre 2004, l’annonce officielle Google Print fait l’effet d’un tremblement de terre. Le nom des cinq bibliothèques est dévoilé. Les “Google 5” possèdent à elles seules près de 50 millions de livres. Google annonce qu’entre 6 et 15 millions de livres seront numérisés en moins de dix ans. L’entreprise de Mountain View dévoile l’existence d’un programme de collaboration avec les éditeurs. Par contre aucun détail n’est donné sur les techniques de numérisation qui seront utilisée, laissant chacun libre d’imaginer les technologies les plus folles. A l’échelle internationale, l’annonce provoque des millions reactions à la fois positives et négatives. Nombreux réalisent qu’il s’agit là d’un phénomène à une autre échelle. 


##  La composition du corpus

Encore aujourd’hui, nous ne savons que très peu de choses sur le corpus des livres numérisés par Google. Une des rares sources d’information est un article écrit en 2005 par Brian Lavoie, Lynn Silipigni Connaway et Lorcan Dempsey, trois membres de OCLC Office of Research. L’article est écrit au début du projet, alors que la numérisation n’a pas encore été effectuée mais se base sur les données de Worldcat, la plus grande base de donnée au monde de notices bibliographiques (55 millions de notice à cette époque, 170 millions fin janvier 2010). L’étude porte sur le fond des cinq premières bibliothèques a avoir participé à Google Books.

L’article met en lumière un aspect très étonnant du corpus numérisé par Google. Intuitivement on aurait pu penser qu’en combinant des millions de livres venant de cinq très grandes bibliothèques, Google aurait produit beaucoup de numérisation en double. Mais les données de Worldcat montre au contraire qu’il y a très peu de  recoupements entre les 5 bibliothèques. 61% des titres ne sont possédés que par une bibliothèque, 20% par 2 bibliothèques, 10% par 3 bibliothèques, 6% par 4 bibliothèques, 3% par les 5 bibliothèques. Le choix de ces bibliothèques initiales a donc été fort judicieux car il permet à Google de couvrir rapidement un nombre très large de documents différents. L’article discute  l’évolution de cette répartition au fil du temps. Plus les livres sont anciens, moins il y a de redondance.

*NOTE : L’article présente d’autres résultats qui confirment la nécessité d’une excellente coordination au niveau mondial pour "numériser tous les livres". Le choix et l’ordre d’inclusion des bibliothèque dans le processus de numérisation est d’une importance capitale. Le coût global et la vitesse de réalisation d’un tel projet peut varier énormément selon la séquence choisie*.

L’article présente aussi de statistiques intéressantes de la répartition des fonds par langue. Au total, 430 langues sont représentées. La moitié des titres sont en Anglais. Le Français à 8% est la troisième langue, juste derrière l’Allemand. Alain Jacqueson note que la taille de WorldCat est passée de 55 à 169 millions, mais que d’après les statistiques données en ligne par OCLC la distribution linguistique reste a peu près identique. On peut faire l’hypothèse que cette distribution linguistique est la même aujourd’hui pour le fond de Google Books. 

##  Le trésor de guerre

L’article de Lavoie et de ses collègues présente aussi une courbe normalisée montrant la décroissance proportionnelle du nombre de titres uniques disponibles selon leur date de parution. Comme on pourrait s’y attendre, les bibliothèques possède beaucoup plus de livres récents que de livres anciens.  Aux Etats-Unis, la date correspondant à la limite des ouvrages sous droits est 1923. D’après la courbe établie par Lavoie et ses collègues, seulement 20% des ouvrages conservées dans les bibliothèques faisant partie du Google 5 sont libres de droits. 

Quel que soit la proportion exacte, le cœur de la base de Google Books est  bien constitué par des livres récents, exploitables commercialement. Pour reprendre les expressions d’Alain Jacquesson, voilà maintenant presque dix ans que Google a entamé la "grande négociation" où l’enterprise tente de monnayer globalement ce "trésor de guerre fabuleux", engrangé grâce à la bienveillance des bibliothèques.

(Ici discussion sur le copyright et la numérisation de masse ?)

Détailler les multiples procès dont Google a fait l’objet pour l’exploitation jugée abusive de ce “trésor de guerre” est hors de notre propos, mais il nous semble intéressant de tenter de s’interroger sur la nature de ce Google a engrangé. De quoi est véritablement constitué le trésor de Google ?

L’objectif de Google est-il de devenir à moyen terme un des plus grands éditeur et diffuser de livres, fort des millions de livres numérisés qu’ils a déjà enregistré sur ses disques durs et dont il peut contrôle strictement l’accès numérique ? En effet, si Google en avait le droit, il pourrait de presque du jour au lendemain proposer la plus grande librairie jamais imaginée. Mais la véritable valeur de ce corpus réside-t-elle vraiment dans la somme de chacun des livres numérisés ou dans le système d’information qu’ils constituent ensemble ? 

##  La logique encyclopédique

Dans son *Traité de documentation* publié en 1934, Paul Otlet prédit : « Sous sa nouvelle forme, le livre sera en croissance continuelle » .  Otlet dans sa clairvoyance imagine déjà la dematerialisation et donc la fluidité de transmission de l’objet. Voici comment il décrit le bureau de lecture : «  la table de travail ne contient aucun livre. Au lieu de cela, il y a un écran et un téléphone. Tous les livres sont éloignés, dans un immense bâtiment (...) » 

*Note : (cité dans (Levie 2006*)).

Seule une forme de standardisation des contenu permet cette fluidité. Il faut littéralement extraire le “contenu” des livres, le documenter sous une autre forme. Ce processus de documentation n’est pas sans rappeler la logique encyclopédique qui tente précisément de redécrire le monde en inventant des formes standardisées. Pourtant les logiques du livre et celle de l’encyclopédie s’opposent presque sur tous les fronts. Le livre imprimé s’organise sous la forme d’un volume fermé, adapté à un discours fini alors que l’encyclopedie se veut un système ouvert, en expansion continue. Le livre invite au parcours linéaire, composé, il a une fonction architecturante. Au contraire, l’encyclopédie propose des accès aléatoires pour englober la complexité du monde. Sa fonction est totalisante. Le livre “en croissance continuelle”, ce n’est pas le livre, c’est l’encyclopédie. 

Ce n’est pas un hasard, si l’encyclopédisme s’est rapidement échappé de la contrainte des volumes imprimés pour s’incarner dans les technologies de réseau. L’ordinateur mondial s’est d’ailleurs constitué sur les principes et les motivations de l’encyclopédisme et son action de description et documentation  s’est appliquée à l’ensemble de la culture. En effet, le principe de l’encyclopédisme est de decomposer tous les objets structurés, pour en extraire le “contenu” sous une forme standard. Un album devient une collection de chansons, un journal une collection d’articles, un roman une collection de chapitres, eux-mêmes décomposables à des échelles plus fines. 

La logique de la numérisation de masse, associée à une chaine de processus d’extraction sous des formes standards correspond précisément à la logique de l’encyclopédisme. Les livres sont en train de devenir une ressource standardisée dans le système encyclopédique global. 

##  Une standardisation sur trois niveaux

Plus précisément, cette standardisation opère sur trois niveaux. Les images des pages de chaque livre numérisé sont transformées en texte par des algorithmes de reconnaissance de caractères, puis mis bout à bout et assembler sous une forme hiérarchique respectant un certain nombre de standard de descriptions maintenant établis à l’échelle internationale. A partir de ces descriptions standardisées, il est possible de produire plusieurs mises en page, pour réimprimer le livre, mais surtout l’afficher sur l’écran d’un ordinateur, d’une tablette ou d’un téléphone. 

Le second niveau de standardisation correspond au repérage dans ces océans de textes de noeuds sémantiques bien identifiés comme des lieux ou des personnes. Ces “noeuds sémantiques” s’organisent ensuite en réseau de relations. Des processus algorithmiques permettent cette extraction sémantique et la mise en réseau de ces informations sous la forme de grands graphes.  Ici aussi des standards sont en train d’être mis en place, notamment dans le cadre du développement du “web sémantique”. 

La standardisation se produit enfin à un troisième niveau pour tenter de capturer l’usage des livres eux-mêmes. De nouveaux formats ouverts sont
aujourd’hui à l’étude pour décrire la manière dont nous progressons dans un livre que nous lisons, les différents chemins de lecture que nous empruntons mais aussi pour enregistrer les notes, les commentaires, les annotations et les autres enrichissement qui nous pouvons apporter en tant que lecteurs. Ainsi, toute la trajectoire de vie d’un livre standardisé  peut elle-même être décrite de manière standard et ces “reading analytics” vont sans doute constituer demain un minerai de valeur exploitable économiquement.

Ainsi, quand nous parlons de numérisation de masse, nous ne décrivons pas qu’un processus de mise en images mais une chaine informationnelle qui tend à la standardisation du contenu de millions de documents. Les “contenus” des livres deviennent des données d’une immense base textuelle inséréw dans un système d’information plus large. Ce nouvel objet informationnel est un “mégatexte”. 

##  Le megatexte comme très long texte structuré 

Le corpus de Google Books peut donc être vu comme un très long texte structuré, un mégatexte,  pouvant donner lieu à toute une série de processus de traitement. Le potentiel computationnel de ce long texte est encore largement sous-estimé. Etre capable d’exploiter ce gisement informationnel en position monopolistique, donne à Google un avantage décisif et fait *de facto* de l’enterprise californienne  un des acteurs culturels majeurs du XXIe siècle. 

Le corpus de Google Books, dont l’expansion se poursuit, constitue sans doute le plus grand megatexte aujourd’hui établi. Mais ce ne saurait être le seul, car Google a d’une certaine manière montré la voie à la constitution de tel macro-objets. Les initiatives pour produire d’autres megatextes se multiplient, motivées par des raisons scientifiques, économiques ou politiques. Il nous faut donc mieux comprendre ce que l’on peut faire avec un tel objet informatique. 

## L’index comme texte “retourné” 

Pour comprendre l’enjeu sous-jacent à l’étude des mégatextes, il nous faut nous arrêter un moment sur un outil intellectuel simple et familier : l’index. Un index est une liste de mots, associant à chaque mots les coordonnées (par exemple sous la forme d’un numéro de page et de ligne) de ses apparitions dans un texte. Un index qui documenterait avec précision la position de tous les mots apparaissant dans un texte permettrait de reconstruire le texte dans son ensemble. Nous pourrions même considérer qu’il s’agit d’une représentation compressée du texte, pouvant occuper moins de place mémoire que le texte lui-même dans la mesure où aucun mot n’est répété deux fois. L’index total est d’une certaine manière une version du texte “retourné”, duale, une autre manière de représenter son contenu. 

La plupart des index qui nous sont coutumiers à la fin des livres ne listent pas l’ensemble des mots d’un texte. Ils document les personnages, les lieux, les concepts. Quand ils traitent de l’utilisation de mots, ils regroupent sous une seule entrée les différentes occurrences  d’un nom qu’il soit au singulier ou au plusieurs ou les différentes version conjuguées d’un verbe. Ce faisant ils extraient des structures et donnent une version encore plus compressées du contenu exprimés dans les textes qu’ils indexent. 

Certains index sont hiérarchiques, lient les entrées les unes aux autres, organisent ontologiquement la connaissance qu’ils cartographient. Ils vont encore plus loin dans la structuration et donc dans la compression du texte qu’ils documentent. 

La constitution d’index joue un rôle clé à la fois dans l’histoire des technologies intellectuelles et plus particulièrement dans le développement de l’informatique. 



(ce que l’on peut apprendre avec un index)

(Voir Umberto Eco, vertige de la liste ?)

(histoire des index)

(IdL : Saint Thomas d’Aquin parle des index de la nature. Les jésuites utilisent les index pour comprendre les mondes. Minier à utiliser les index). 

(Patrologia grec et latin par les jésuites)

(Saint Thomas d’Aquin est la figure clé pour les jésuite) 

(> Domincains. Saint Thomas d’Acquin grand occulte. Cooncordinace sur les choses. )

(INDEX en Biologie)

Le prêtre jésuite Roberto Busa est reconnu comme un des pionniers de la linguistique computationnelle, notamment par le travail qu’il entreprit dès les années 1940 pour l’utilisation de l’informatique pour la constitution d’index. En 1949, il entreprit une collaboration avec la société IBM. L’objectif de Busa était de produire informatiquement un index de l’oeuvre de Saint Thomas d’Aquin, *Index Thomesticus*, pour pouvoir analyser et comprendre différemment son oeuvre. L’ensemble devait pouvoir ensuite être imprimé sous forme d’une collection de volumes papier. Produire un index associant à chaque sequence de lettres séparé d’un espace, l’ensemble des positions d’apparition était relativement simple. Mais Busa souhaitait produire une index de concordance dans lequel la version “lemmatisée” des mots était produite. Le nom ou le verbe devaient par exemple être reconnus comme identiques qu’ils soient au singulier ou au pluriel. Reconnaitre les différentes façon de nommer un personne ou un lieu posait aussi un problème spécifique pour la constitution d’untel index. Busa commença à travailler sur des algorithmes capable de résoudre de manière semi-automatique ces problèmes d’indexation. Il y consacra l’ensemble de sa vie. 

## Comment créer un index contenant des milliards de mots ?

La construction efficace et le stockage d’index permettant de repérer la position des mots dans des textes de plus en plus long est depuis le début de l’informatique un défi technologique majeur et structurant. A la fin des année 1940,  il fallait construire ce type d’index au moyen de cartes perforées. Pour réaliser son projet, Busa devait produire 13 millions de ces cartes, une pour chaque mot du corpus, associant les contexts d’apparitions sous la formes de 12 lignes imprimées à l’arrière. Mises bout à bout, ces cartes auraient mesurés 90 m de long, 1.2 m de largeur, 1 m de profondeur et pesait aux alentours de 500 tonnes.  L’invention en 1955 des bandes magnétiques changea heureusement les dimensions du projet. Grace à cette invention, il ne fallait plus que 1800 bandes, dont la longueur combinée correspondant à 1500 km. En 1980, quand le projet touchait à sa fin, tout l’index put tenir sur 20 bandes. Puis en 1992, Roberto Busa put tout faire tenir sur quelques CD-ROMs. 

Mais alors que les méthodes de stockages et d’accès faisaient des progrès exponentiels, la taille des index potentiellement créables ne faisait que croitre. C’est d’ailleurs les défis liés à la constitution d’index de très grande taille qui a conduit Google à développer une de ses plus importantes innovations technologique. En effet, au coeur du moteur de recherche de Google, il y a essentiellement un grand index qui associe à chaque mot une liste de pages qui contiennent ce mot. Dans ce grand index le mot “Venise” est par exemple associé à tous les pages qui contiennent ce mot. Evidemment, la difficulté est que cet index est immense, contenant des milliards de mots dans des centaines de langues. Un tel index peut facilement représenter des dizaines de teraoctets de données. Plus il y a de pages à indexer plus il faut de place pour stocker l’index, et plus longue sera la requête pour chercher un mot particulier. De plus, au fur et à mesure que le nombre d’utilisateurs croit, le nombre de requête à effecter chaque seconde augmente. Si notre grand index était stocké sur un seul grand serveur, il rencontrerait rapidement de sérieux problèmes. 

Le grand index de Google doit nécessairement être stocké sur un nombre potentiellement très grand de machines distinctes, mais il doit en même temps être lisible très rapidement. La solution à ce problème est l’utilisation d’une fonction dite de “hachage” capable d’associer à une chaine de caractère un nombre particulier. Imaginons qu’au lieu d’utiliser un gros serveurs, nous avons à notre disposition 1000 machines plus modestes. Considérons une fonctions H qui associe à un mot *m*, un nombre entre 1 et 1000. Cette fonction de hachage peut par exemple choisir ce nombre au hasard, tant que l’association entre le mot m et le nombre produit reste constante dans le temps. Le mot “Venise” sera par exemple systématiquement associé au nombre 742. Grâce à cette fonction, nous décidons que la responsabilité de ce mot sera donnée à la machine 742. Si un robot qui parcoure le web trouve une page qui continent le mot “Venise”, il en informe le serveur 742 qui stocke l’information dans son index. Si un utilisateur tape ce mot, le serveur 742 est de nouveau réquisitionné. Grace à ce système, le poids du stockage mais aussi les requêtes sont équitablement répartis entre les 1000 machines. Si ces 1000 machines sont insuffisante par rapport au nombre de pages à index ou au nombre de requêtes des utilisateurs, il suffit de rajouter des nouvelles machines. Google utilise ainsi des “fermes” contenant des milliers de machines. Grâce aux fonctions de hachage, la construction d’un très grand index est un problème hautement “parralélisable”. C’est cette propriété qui a permis la construction et surtout la croissance continue des immense index qui permettent aujourd’hui de retrouver les informations sur le web. 

## L’ombre d’un mégatexte

Jean-Baptiste Michel et Erez Aiden furent parmi les premiers a avoir tenté de mener de recherche sur le corpus de Google Books. Ils avaient l’intuition que de nombreuses informations pourraient être extraites des millions de livres numérisés par Google. Sur le principe, les responsables du projet chez Google étaient favorables à ce que des recherche de ce types soient effectuées sur le corpus, mais ils était inquiets pour les questions de droits d’auteurs. Etant donné l’enjeu de la grande négociation qui opposait Google aux éditeurs, ils ne pouvaient prendre aucun risque légal en laissant librement des chercheurs du monde académique accéder au contenu des livres numérisés. Comment permettre des recherches sur cet immense corpus sans donner directement accès au “trésor de guerre” ? 

Michel et Aiden, conscients que Google ne leur donneraient pas directement accès au contenu textuel du mégatexte , proposèrent de calculer une “ombre” du corpus, c’est à dire une projection sous un certain angle qui permettrait d’effectuer des recherches sans pour autant donner accès au contenu des livres. Cette ombre serait constituée à partir d’un certain nombre d’index calculant les apparitions de séquence de mot dans l’ensemble corpus. Une séquence de *n* mots consécutifs s’appelle un *n-gramme*.   Le mot “Venise” est par exemple un 1-gramme et l’expression “United States of America” est un 5 gramme.  

Soucieux que l’on ne puisse pas reconstituer le megatexte à partir de son ombre, Michel et Aiden prirent deux précautions. Ils construisent un outil qui ne donnaient pas accès directement aux index, mais seulement aux fréquences d’apparition des n-grammes, années par années. Ils supprimèrent de la base toutes séquences de mots qui n’apparaissaient que très peu de fois dans le corpus.  Ces précautions garantissaient que les contenus de chaque livre numérisé seraient inaccessible aux chercheurs mais ne limitaient pas les études menées pour comprendre les grandes évolutions culturelles que le corpus de Google Books pourrait révéler. L “ombre” du mégatexte était incomplete pour le recomposer mais suffisante pour en analyser les grandes tendances. 

Michel et Aiden construirent un outil simple pour visualiser l’évolution de l’utilisation de certains mots dans le corpus, en comptant combien de fois ces mots apparaissaient chaque année en proportion relative par rapport à l’ensemble des mots présents dans le corpus cette année là. Le corpus étudié portait sur 5 millions de livres numérisés par Google, un sous ensemble du corpus total choisi en fonction de la qualité de leur transcription. Selon Michel et Aiden, ce corpus représenterait 4% du nombre total de livres jamais publié. La base correspond à un total de 500 milliards de mots,  361 en Anglais, 45 en Francais, 45 en Espagnol, 37 en Allemand. Les livres les plus anciens datent de 1500, mais le gros du corpus étudié est constitué par les livres des deux cents dernières années. 

L’outil fut publié simultanément à la parution d’un article dans Science qui présentait un ensemble de premiers résultats exploratoires. En utilisant la base, les auteurs purent estimer la taille du lexique Anglais (tous les mots dont la fréquence est supérieur à 1 pour 1 milliard) à environ 500 000 mots en 1900 et montrer que depuis 50 ans, le lexique ne fait que croitre, avec environ 8500 mots nouveaux par ans. 




Publication dans Science et diffusion de l’outil

Excitation de l’outil. Bien contre mal. Vert contre rouge. Homme contre Femme. 
Exploiration juvenile. 

Addiction.

## Distribution temporelles 



Mais plus sérieusement … 

- burn / burnt
- Mesure de la renommée
- Detection de la censure : La courbe à deux cloches. 
- La courbe de l’oubli : Année, Evenement. Demi-vie. Nous persons l’interet dans le passé de plus en plus vite. 
- 




## L’ombre de Google Books

L’index utilisé par Michel et Lierberman ne cartographiait pas que les appariations de mots mais également les apparition de séquence de n mots consécutifs. 
(Pourquoi cette intuition ? Lien avec la biologie ?)
(Se rappeler que les n-grammes contiennent la fonction du mot … Analyse syntaxique > Analyse sémantique … Idem pour l’architecture)


Cet outil permettait de projeter l’index sous forme de courbes. 



#  Comment lire 200 ans d’articles de journaux. 

##  Une aventure helvétique

##  Le champ attentionel des medias

##  Une typologie des evolutions 

##  Event stream, detection d’entité nommées

## La presse numérisée comme mégatexte

La technique mise en place par Michel et Aiden est utilisable sur n’importe quel megatexte s’étalant chronologiquement sur plusieurs années. Les corpus de presse numérisé se prêtent parfaitement à ce type d’analyse. Alors que les livres numérisés par Google constituent un corpus dont la composition est relativement hétéroclites, les corpus de presse sont de grand ensemble de textes dont la nature est beaucoup mieux structurée et comprise. Google s’est intéressé à ces corpus mais sans avoir encore réussi aujourd’hui à conclure des accords avec les ayants droits pour les exploiter systématiquement. L’accès et l’exploitation de ce types de megatexte est un enjeu majeur pour le XXIe siècle. 

Parmi les pays européens, la Suisse a été un des premier acteur à se lancer dans la numérisation à grande échelle de grand corpus de journaux. Un accord entre les différents ayant droit a permis d’éteindre la numérisation jusqu’aux années 2000, sans se limiter aux titres passées dans le domaine publique. C’est ainsi que le journal de Gènèeve ou la Gazette de Lausanne ont pu être entièrement numérisés et transcrits, constituant un corpus regroupant plusieurs millions d’articles de presse s’étalant sur presque deux cents ans. Mis bout à bout, ces pages de journaux représenteraient plusieurs kilometres de documents. Une fois numérisées elles constituent un mégatexte de plusieurs milliards de mots.

Ces articles de presse racontent à la fois à la grande histoire du XIX et XXe siècles, mais aussi un certain nombre d’épisodes d’histoire locales, chroniquées mais aujourd’hui oubliée. La presse numérisée incluent également des rubriques spécifiques très riches en information comme les petites annonces, les cours de la bourse, la météo ou les horaires de train. C’est une mine d’information exceptionnelle pour toute personne voulant explorer une chronique dense de l’histoire locale et globale. 

La transcription et l’indexation de ces corpus constituent une première étape qui permet de les rendre cherchables. Le corpus devient navigable comme l’ensemble des pages du world wide web. Tous les articles mentionnant une personne ou un lieu particulier peuvent ainsi être retrouvés. Mais, comme dans le cas de Google Books, ces mêmes index peuvent être utilisés pour identifier des motifs culturels globaux. 

## Comparaison de megatextes et champ attentionnel d’un média. 

Un corpus de presse est différent de celui Google Books dans la mesure où il caractérise un média spécifique avec ses caractéristiques particulières, alors que les livres numérises ne constituent qu’une masse hétéroclites, non structurée. En étudiant un corpus numérisé d’articles de presse nous pouvons non seulement étudier l’évolution culturelle mais aussi l’évolution du média lui-même. En comparant systématiquement comment deux medias rendent compte des évènements du monde nous pourrons tenter de caractériser finement leur biais. 

L’index d’un corpus de presse numérisé renseigne directement sur le champ attentionel de ce média. Quelles sont les lieux et les personnes les plus citées ? Quel rapport le média entretient-il avec le futur et le passé ?

Les toponymes de la region genevoises sont évidemment bien plus présents dans le corpus numérisé du “Journal de Geneve” que dans celui de la “Gazette de Lausanne”. Le focus attentionel du média est centré spatialement sur une certaine forme de localité. Mais le rapport à l’internationel est également différent. 


## La recherche par motifs culturels (recherche par courbe)


Motivation : verbes irregulier

Un problème se posait néanmoins : comment effectuer ces recherches sans leur donner directement accès au trésor. 

Une ombre permettant de faire des recherches mais pas de reconstituer les textes. 

LA CONSTITUION d’INDEX a CONDUIT GOOGLE A FAIRE UNE DES CES PLUS IMPORTANTES INVENTION TECHNIQUE


(Aujourd’hui très grand Index). 

Pourquoi construire de tels index ?

Mots frequents - La langues
Les Mots rares - Le contenu

Les index consiste en une forme particulière de pliage de textes. 

LIEU + DATE. Busa et l’index de Saint Thomas D’Aquin. 

Jean-Baptiste Michel et Eredz furent parmi les premiers a avoir tenté de mener de recherche sur le megatexte de Google. 

Motivation : verbes irregulier

Un problème se posait néanmoins : comment effectuer ces recherches sans leur donner directement accès au trésor. 

Une ombre permettant de faire des recherches mais pas de reconstituer les textes. 

Serge Abitebul p.61

## La littérature numérisée comme mégatexte

## Index et sémantique

Latent Semantic Analysis > pour faire une distance entre les mots
Champs attentionnel d’un mot > Jeux Olympique

## La linguistique repensée








##  Autocorrelation et formes

Musique,
Mots,

Les formes apparaissent. 

Cf Cocco. 

##  Traduction automatique et grands nombre

Dès le début de la guerre froide, les américains prenaient très au sérieux le défi de la traduction automatique. 

p. 37 Big data


La défi de la traduction automatique est presque aussi ancien que l’informatique. 

L’Anti Babel

##  Le grand graphe










#  Comment étudier la littérature par les mathématiques ?

#  Comment étudier des romans que l’on pas lu

#  Comment étudier la littérature mondiale / à grande échelle

Character system

Alex Woloch

Sur Rousseau et De Roulet. 

##  Que peut-on dire d’un livre sans l’avoir lu

Franco Moretti. 

##   Alex Woloch and Character spaces

Franco Moretti. 

##   Visualiation

Lord of the Ring xbci

##  La centralité positive de Mme de Warrens et le meta-personnage du complot. 

Pour comprendre la variété des traitements numériques qui s’appliquent aux megatextes, il peut être utile de considérer un exemple à une plus petite échelle. Le mathématicien Yannick Rochat a effectué une analyse qui ne porte que sur une seul oeuvre, les confessions de Jean-Jacques Rousseau, mais dont le principe général peut évidemment être décliné sur n’importe quel corpus muni d’un index. Il s’agit une fois de plus de voir ce qu’on peut analyser d’une oeuvre sans la lire. 

L’analyse se base sur une hypothèse simple : si deux personnages sont plusieurs fois évoqués ensemble dans le récit nous pouvons en déduire qu’ils sont d’une manière ou d’une autre liés. A partir de l’index de l’oeuvre et en supposant que deux personnages sont potentiellement liés s’ils apparaissent un certain nombre de fois dans des pages adjacentes, Rochat construit un grand graphe qui lie environ 300 personnages des Confessions. Il peut alors appliqué un certain nombre d’outil de la théorie des graphes pour étudier la structure de ce graphe et le rôle de chacun des noeuds, c’est à dire chacun des  personnages, qui le constitue. 

Rochat s’est en particulier intéressé à une famille de mesures, regroupées autour du terme générique de centralité, et qui évalue de différentes manière l’importance d’un noeud au sein du graphe global. 

Depuis un texte de revue devenu référence en la matière (Freeman, 1979), la centralité est vue comme un concept quantifiant l'importance des sommets d'un réseau en regard de diverses propriétés mathématiques pouvant être exprimées en fonction de traits caractéristiques dans la constitution de groupes sociaux. Nous illustrons ceci en présentant les indices principalement utilisés dans ce domaine et par extension dans cette étude.

Archetype de personnage

Différentes centralité. 

La littérature eclaire les mathématiques, les mathématiques éclairent la littérature. 


##  L’atelier d’écriture de Voltaire 

Voltaire

##  L’organisation naturelle de l’encyclopédie

Voltaire

##  J-B Michel et Zip’f law

# Gromov, les calendriers et les espaces Riemmanien (cf. p.40)

# Comment  organiser tous les savoirs du monde. 

##  Retour sur le futur des livres …

Notre monde a changé d’échelle. Il est traversé par des flux informationnels sans précédents dans l’histoire de l’humanité. Les utilisateurs de Facebook partagent environ 10 millions de nouvelles photos par heure. Sur YouTube, une nouvelle heure de vidéo est mise en ligne par seconde. Chaque jour, près de 400 millions de nouveaux tweets  documentent minute par minute la pulsation du monde. Le système informationnel que ces différents flux irriguent ne se cesse de croitre, constituant une base documentaire sans précédent sur notre époque.  Ce déluge de données a ouvert un champ de recherche nouveau: les “Big data”. Stocker, indexer et repérer des structures dans ces flux est un des enjeux majeurs des premières décennies du XXIe siècle. 

Le monde est aussi devenu 



# Interlude : Elements mathématiques pour une linguistique des grands nombres

Au printemps 2010, L’exposition “Mathématique, un dépaysement soudain” a la Fondation Cartier mettait à l’honneur les travaux du mathématicien franco-russe Mikhaïl Leonidovich Gromov. Pour rendre tangible le monde de Gromov, le cineaste David Lynch s’était fait plasticien et avait imaginé une véritable architecture invitant à la déambulation dans l’espace topologique. Chaque pièce correspondait à une étape de la pensée de Gromov. En contrepoint, au sous sol, des interviews de Raymond Depardon donnait à voir et à entendre, ces chercheurs des espaces abstraits. Toute l’exposition ne disait qu’une chose : Les mathématiques sont une invitation au voyage,  une opportunité pour percevoir le monde différemment. 

*NOTE : Mon équipe a participé à plusieurs projets pour l’exposition. C’est mon collègue Pierre-Yves Oudeyer qui m’a entrainé dans l’aventure et suggéré mon nom au directeur de la fondation, Hervé Chandès et à Michel Cassé, commissaire de l’expositon. En effet, il y a dix ans Pierre-Yves et moi-même avions travaillé ensemble sur un principe mathématique très proches des théories de Gromov, une méthode inspirée de la curiosité des jeunes enfants permettant d’explorer intelligent des espaces de données très grands et très bruités. Nous ne connaissions pas les travaux de Gromov à l’époque et c’est avec grande surprise que avons découvert qu’il s’attaquait depuis plusieurs années à un problème similaire, mais d’une manière beaucoup plus formelle et systématique. Cette coincidence faisait de nous de bon passeurs pour faire le pont entre les théories de Gromov et leur reinvention par David Lynch. Ce fut pour moi l’occasion de me plonger dans les écrit de ce penseur de l’espace si particulier*.

Gromov est un géomètre du plissement. Il perçoit les formes du monde et s’efforce les replier de manière inédite. Il plisse les yeux et voit des structure que notre vision trop nette peine à distinguer. Cette capacité lui permet de ramollir certains concepts trop rigide pour les déformer, les rendre un peu imprécis tout en conservant leur substance et les rendre ainsi applicables à d’autres domaines. Cette manière de voir des structures topologiques est caractéristique du style “Gromovien”. 

*NOTE:  Une de ses plus grandes découverte est le “h-principe”. Nash, avait démontré dans les années 50 que l’on peut dessiner n’importe quel espace muni d’une métrique de Rienmann dans un espace euclidien, à condition d’avoir suffisamment de place, c’est-à-dire que l’espace ait suffisamment de dimensions. Dans les années 70, Gromov formule le h-principe qui simplifie grandement les preuves théorique de Nash et en étende la portée*.

Gromov s’est intéressé à de nombreux domaines, en particulier lié aux sciences de la nature, mais c’est sa vision très particulière de la linguistique des grands nombres qui est particulièrement pertinente pour notre propos. Gromov a vision géométrique de la texture des textes, très peu orthodoxes par rapports aux approches linguistiques classiques : Gromov voit les textes se replier dans l’espace. 

Pour tenter de comprendre l’originalité de cette approche, sans pour autant nous lancer dans des raisonnements mathématiques complexes, il nous faut d’abord considérer quelques ordres de grandeur. Une page d’un livre imprimé contient entre 2000 et 3000 caractères. Un livre de 400 pages contient entre 8. 10^5 et 12.10^5 donc disons pour simplifier 10^6, c’est-à-dire un million de caractères. Disons que nous pouvons lire dans un vie,  1000 livres, donc 10^9, un milliard de caractères. La librairie du Congrès contient environ 32 millions de livres. Donc 1 Librairie-du-Congrès = 32 10^6 livres = 32 10^12 caractères. 

Maintenant considérons un alphabet de 32 caractères contenant aussi les espaces et  les signes de ponctuations. 32 est nombre très agréable pour nous calculs car la racine carrée de 1000 est 31.6.. ≈ 32. Imaginons que les textes ne soient composées que de suites aléatoires de caractères. C’est absurde, mais cela va nous permettre de mieux sentir la structure de cet immense espace. Dans ces conditions, un caractère *a1* de l’alphabet se retrouverait en moyenne tous les 32 caractères. Une chaîne de deux caractères *a1a2* tous les 32×32 ≈ 1000 caractères, toutes les demi-pages. Une chaîne de trois caractères *a1a2a3*, tous les 32^3 ≈ 3.10^4 caractères Une chaîne de quatre caractères a1a2a3a3, tous les 32^4 ≈ 10^6 caractères, c’est à dire au mieux deux ou trois fois dans  1 livre de 400 pages.
Une chaîne de six caractères qui ne se retrouve aléatoirement que  tous les 32^2 . 10 ^6 ≈ 10^9 caractères, ne sera donc vu qu’une ou deux fois dans une vie entière de lecture. Enfin une chaîne de neuf caractère ne se retrouvera peut-être qu’une seule fois dans toute la Libraire du Congrès.

Évidemment, nos textes ne sont pas du tout des suites aléatoires de lettres. Ils sont extrêmement structurés, syntaxiquement et sémantiquement. L’objet de la linguistique est précisément l’étude de ces structures sous-jacentes. En d’autres termes, le linguiste tente de décrire ces long objets linéaires que son les textes et les paroles de manière plus compacte en montrant qu’ils possèdent des régularités. En étudiant les arbres syntaxiques et les règles de transformation grammaticales, ils décrivent en fait des algorithmes de compression. 

Misha Gromov aborde ce problème de compression avec un angle différent, en proposant une analogie qui lui vient sans doute de ses travaux sur les structures en biologie. Appelons *L* une langue définie par l’ensemble des textes écrits (livres, pages internet, etc.) dans cette langue.  Evidemment, un tel ensemble n’existe que dans notre pensée, mais cela suffisant pour l’instant. Il est théoriquement envisageable, mais s’il est difficile à réaliser en pratique. La première étape pour compresser/plier/décrire la langue *L* est de tenter de la transformer en un espace doté d’une métrique. 

(La suite est sans doute à simplifier et passer partiellement en note)

Si un espace peut être muni d’une métrique, s’il devient “métrisable”, nous pouvons effectuer des raisonnements géométrique et tirer des conclusions qui ont des points communs avec ceux que nous effectuons intuitivement lors que nous réfléchissons sur les distance dans notre espace familier. Pour qu’une mesure sur un espace soit une une métrique, nous nous attendons par exemple à ce que si la mesure  entre deux moins est nulle, ces deux points soient les mêmes ou que la mesure d’un point *x* à un point *y* soit symétriquement la même que celle d’un point y à ce même point *x.*  Tout ceci est relativement simple. Mais la propriété la plus importante est ce que nous appelons “ l’inégalité triangulaire” : Nous nous attendons également à ce que la mesure d’un point *x* à un point *y* soit toujours inférieur ou égale à la somme des distance entre ce point x et un point intermédiaire *z* et un point intermédiaire *z* et le point *y*. 

Pour construire une métrique sur l’espace des textes, Gromov propose de considérer par exemple l’espace des chaque chaînes de caractères de longueur l. Chaque point x de cet espace X est donc une chaîne (a1,a2,a3,… al). Dans son exemple, l prendrait typiquement une valeur entre 10 et 30, soit une petite séquence de mots de nos langues naturelles. Pour aller au plus simple, il propose de définir la distance entre deux point x = (ai) et y = (bi) en fonction du nombre k = k(x,y) de segments initiaux communs aux deux chaînes. Cela veut dire que ai=bi pour i=1,2…k.

Une formulation de la distance entre x et y peut être la suivante : dist(x,y) = epsilon ^k où 0 < epsilon < 1. Prenons par exemple,  epsilon = 1/2 et l = 16 et ignorons tout problème lié aux espaces et la ponctuation. Les deux chaines "abcdefghifgklmn" et "xydfdsfseerfsadf" ne partagent aucune sous chaine en commun, leur distance est donc epsilon^0 = 1. Si les deux chaines avaient une lettre en commun (“abcdefghifgklmn” et “aydfdsfseerfsadf”), la distance serait  epsilon^1 = 0.5. Pour deux chaines beaucoup plus similaires, partageant par exemple 12 éléments en commun "leschatssontgris","leschatssontnoirs”, la distance est de  epsilon^12 = 0.0002. 

C’est bien une métrique car elle satisfait les conditions classiques que nous avons évoqué plus haut.  d(x,y) = 0 ssi x = y, d(x,y) = d(y,x), d(x,z) =< d(x,y) + d(y,z).

L’espace *X* des chaines de caractères semble aussi grand que *L*, puisqu’il contient autant de points de caractères dans *L* .Mais le concept de localité s’applique sur cet espace. Imaginez vous suivant une chaine de caractère de longueur l au sein d’un texte dans l’espace X. A une faible distance de cette chaîne se trouvent d’autres chaines appartenant au même texte ou à des textes différents, ce sont les différentes continuations directe de la chaîne sur laquelle vous êtes. Dans l’espace *X*, les textes sont repliés, formant des structures complexes. 

Gromov compare ce processus au passage d’un polypeptide à une protéine. Les protéines se replient sur elles-mêmes pour former une structure tri-dimensionnelle qui définit leur fonction. De la même manière, les formes repliées des textes dans *X* témoignent de la structure syntaxique et sémantique d’une langue donnée. Il est sans doute possible de visualiser en deux ou trois dimension ce phénomène de repliement d’un texte ou d’un ensemble de textes. Ce type de méthodes pourraient ici permettre de mettre voir comment les textes écrits dans une langue donnée se replient sur eux-mêmes montrant ainsi la structure sous-jacente de la langue dans lesquels ils sont écrits. Ceci ne semble pas forcement extraordinaire, mais la force de ce type d’approches géométriques est qu’à aucun moment elles ne supposent un quelconque savoir syntaxique ou linguistique. Toutes ces procédures sont extrêmement générales. C’est simplement de la géométrie.

(NOTE : A lire Gromov, on se met donc à rêver à des visualisations de multitudes de textes repliés et à tenter d’imaginer ce que nous pourrions y voir. Pourrions-nous deviner leur langue juste par la structure des figures qu’ils forment ? A l’inverse, certaines propriétés géométriques sont-elles communes à toutes les langues ? Apprendre une langue ne consiste-t-il pas simplement à connaître à un certain niveau de détail la structure des ces figures repliées. Le jeune apprenant n’en connaît-il que les grandes lignes alors que celui pour qui s’est la langue natale en maîtrise avec la structure fine ? Autrement dit combien d’exemples de phrases faut-il avoir rencontré pour que la figure géométrique résultante dans l’espace X commence à ressembler à celle produite par tous les énoncés d’une langue donnée ?)




# Comment gagner des milliards simplement en vendant des mots ?

*Je reviens sur ce qui constitue le cœur du modèle commercial de Google, la vente des mots, et discute comment deux algorithmes permettent de produire plus du 50 milliards de dollars de chiffre d’affaires par an. Je m’interroge ensuite sur les conséquences de ce nouveau capitalisme linguistique sur la langue elle-même et sur le nouveau lien intime qui lie la médiation algorithmique et l’expression langagière. Je dessine ainsi les contours d’une linguistique des grands nombres utilisant les algorithmes comment moyens d’étude critique de l’évolution de la langue* ? 

Google est la première entreprise  à avoir veritibelment saisi que le changement d’échelle conduisait à des logiques commerciales radicalement nouvelles.



# Comment devenir le grand maître de Wikipedia ?

*Après être revenu brièvement sur l’histoire de Wikipedia et la manière dont cette encyclopédie a détrôné les encyclopédies classiques, je déconstruis le mythe de Wikipedia comme le résultat d’un simple processus d’intelligence collective et montre qu’il s’agit essentiellement d’un immense jeu. Je discute de manière plus générale le rôle des processus ludique dans la construction de système de connaissance partagée puis tente de démontrer que Wikipedia peut se comprendre comme un jeu de rôle massivement multijoueurs, comme World of Warcraft. J’explique les règles de ce jeu et la manière de progressivement gravir les échelons pour pouvoir espérer un jour devenir grand maître de Wikipedia*. 

Wikipedia incarne le rêve d’une encyclopédie à grande échelle, libre et gratuite où chacun pourrait contribuer. Les centaines de milliers d'articles écrits collaborativement semblent démontrer que la culture numérique ne produit pas que de l’aliénation et de nouvelles opportunités commerciale mais aussi parfois du bien commun. Les internautes tiennent à Wikipedia et c’est pour cette raison que cette immense encyclopédie est des resources les plus susceptibles de survivre au passage des années. Dans les prochaines décennies, des géants économique comme Google ou Facebook pourraient un jour tomber à l’occasion d’un hiver économique mettant à mal leurs modèles commerciaux. Leur chute risquerait d’entrainer la disparition des énormes quantités d’information que les internautes leur ont confié. Wikipedia est au contraire sauvegardé sur des milliers d’ordinateurs de particulier qui parce qu’ils y participent ont décidé d’en assurer de manière décentralisée la conservation. 

Wikipedia semble apparemment incarner l’exemple archetypique de la “sagesse des foules”, qui selon les quatres critères de Surowiecki consiste en l’harmonieuse combinaison de la diversité (chacun participants a sa propre histoire et culture), de l’indépendence (chaque participant prend ses propres décision), de la décentralisation (pas de centralisation globale) et et des effets d’aggrégation collective (une procédure pour arriver au consensus). Wikipedia semble organiser de manière ouverte la production collective de connaissance, selon un modèle apparemment absolument inédit. 

# Une idéologie ?

L’encyclopédie en ligne n’en reste pas moins un objet singulier. Dans notre culture politique qui sépare les idées et les programmes sur l'axe gauche-droite, Wikipedia porte à confusion. Parce qu'elle vise à créer du bien commun numérique, sous le régime de la gratuité et selon une logique collective et communautaire, l'encyclopédie en ligne est un projet qui partage des valeurs typiquement associées à la "gauche". Certains pourraient y voir la réalisation concrète par la technologie d'une utopie au parfum marxiste : un collectivisme qui fonctionne.

Mais en réduisant les structures de contrôle à leur plus strict minimum, en prônant la résolution décentralisée des conflits, en favorisant l'émergence d'une communauté "plate" où experts et amateurs sont mis sur un pied d'égalité, Wikipedia est aussi empreinte d'une idéologie profondément libérale, souvent associée, en Europe, à une pensée de "droite". Le succès de l'encyclopédie se base sur la promesse d'un progrès de la connaissance grâce à une autorégulation du marché des contributions libres. Les individus qui contribuent suivent des buts personnels, parfois concurrents, mais ils concourent à un intérêt général. La concurrence libre des contributions conduit à l'élimination progressive des erreurs. Marc Foglia dans son livre sur Wikipedia (Editions Fyp, 2008) fait le parallèle avec la théorie développée par Adam Smith au XVIIIe siècle. Tout comme en économie, c'est une "main invisible" qui guiderait Wikipedia vers des informations toujours plus fiables.

Beaucoup d'universitaires qui soutiennent la création de biens communs numériques, la publication en format ouvert et un accès démocratique aux ressources en ligne restent mal à l'aise face à ce libéralisme des idées qui va à l'encontre d'un savoir garanti par des instances centralisées comme les universités. Certains tentent de placer le débat sur le plan des performances, les uns voyant dans Wikipedia la démonstration du succès effectif de l'autorégulation, les autres pointant du doigt les ratés, les faiblesses et les dérives de l'encyclopédie  collaborative pour en démontrer les limites. Mais, comme en économie, il s'agit peut-être finalement d'une question de conviction profonde sur les vertus et les vices de la concurrence libre et l'existence hypothétique d'une "main invisible".

Marché libre des connaissances. 

Open Street Map

Jeu et connaissance

## Créer des connaissances à son insu

Depuis quelques années, nous avons vu plusieurs réussites de création collective de connaissance utilisant des jeux. Un exemple emblématique est peut-être le jeu Foursquare. Par sa dynamique ludique et la dizaine de millions d’utilisateurs, Foursquare a réussi à documenter l’espace géographique de manière sémantique. 

Le principe est simple. Dans son concept initial, Foursquare propose aux joueurs de devenir “maire” d’un lieu où il se trouvent. Grace à l’application mobile, les joueurs peuvent regarder les lieux autour d’eux et faire un “check-in” dans de ces endroits, par exemple, le café où ils sont en train de prendre un verre. Si le lieu n’existe pas dans la base de données, ils peuvent le créer. C’est par ce procédé que petit à petit, de nombreux lieux deviennent documentés alors qu’ils ne figurent pas sur les cartes officielles. Foursquare a pu proposer cette immense et précieuse base de données à d’autres services, comme Instagram.

Foursquare est un exemple minimal de jeux en réalité alternée qui laisse une grande liberté au joueur. Le monde se documente comme un effet secondaire de leur peregrination.  Mais il existe des jeux plus complexes construits autour de scénarios, le plus souvent participatifs, qui se déroulent dans notre monde et en temps réel. Ingress, développé par Google, est par exemple un jeu dans lequel les participants doivent effectuer des actions dans le monde réel en se rendant à des endroits particuliers. Aujourd’hui les grandes entreprises comme Google accumulent des données sur le comportement des utilisateurs, mais jusqu’à présent elles n’avaient pas d’influence directe sur leurs actions. Avec un jeu comme Ingress, une entreprise peut littéralement « sculpter » comportement des joueurs en les invitant par exemple à se rendre à un point particulier pour effectuer une action spécifique. Ceci peut servir à améliorer la collection de données cartographiques, notamment pour Google maps, en documentant les déplacements des joueurs pour se rendre vers ce point, une manière d’affiner l’évaluation des temps de parcours et la diversité des trajets possibles. C’est une forme spécifique et efficace de « crowdsourcing » qui permet non seulement de recueillir de très large quantité de données, mais de se servir des joueurs comme sujets d’expérience. Celui qui contrôle le jeu transforme de fait les joueurs en pantins.

Les jeux peuvent créer des bases de connaissances. Mais nous pourrions également argumenter que les grands services de créations connaissances partagées sont également des jeux même s’ils ne sont pas présentés comme tels. Je vais tenter de développer cet argument autour de trois exemples : Twitter, Quora et enfin Wikipedia.

## Twitter est un jeu

Dans leur livre *Gamification by Design*, Gabe Zichermann et Christopher Cunningham décrivent minutieusement le concept d’Onboarding. L’Onboarding désigne l’initiation d’un utilisateur novice à un système complexe comme un jeu ou un service en ligne. Le nouvel utilisateur doit comprendre progressivement comment le système fonctionne et surtout avoir envie de continuer à l’utiliser dans le futur.

Une stratégie possible consiste à créer une boucle d’engagement social. Pour Zichermann et Cunningham cette boucle se décompose en quatre temps. Une émotion motivante (1) rencontre une proposition d’action (2)  qui donne des effets visibles incitant à continuer l’exploration (3) et conduit à des progrès mesurables (4) qui a leur tour renforcent une émotion motivante (1′).

Pour l’utilisateur novice de Twitter, tout commence avec probablement avec la curiosité d’essayer le service dont il a entendu parler (1), il découvre qu’il peut rédiger des Tweets (2). L’utilisateur quitte en général la plate-forme à ce stade sans savoir vraiment si elle va utiliser le service de manière régulière. Avec un peu de chance dans les heures ou les jours qui suivent, quelqu’un le mentionne dans un Tweet (3) (Il voit son nom avec @ et en déduit l’usage de ce signe), ce qui incite le novice à revenir sur le service et à continuer la conversation. Si ses Tweets sont jugés pertinents, l’utilisateur commence à avoir des followers. Cette mesure explicite (4)  de son importance dans Twitter le motive à continuer à tweeter avec pour objectif plus ou moins avoué de tenter d’avoir plus de followers.

Au fur et à mesure que l’utilisateur de Twitter devient un expert, sa perception de la boucle d’engagement se modifie et l’importance des divers classements associés au service de micro-message se précise. L’expert comprend qu’un grand nombre de  mention et de retweet peuvent modifier son score sur un  des services qui proposent aujourd’hui d’évaluer le capital social (par exemple Klout). Il comprend les meilleures heures pour Twitter. Consciemment ou inconsciemment il optimise son écriture et la temporalité de ses messages pour jouer au jeu Twitter de la meilleure manière possible.

## Quora est un autre type de jeu

Zichermann et Cunningham analyse de la même manière la structure ludique de Quora. Lancé à grand bruit en 2009, Quora est un service de questions/réponses. Son objectif affiché est de proposer un marché de la connaissance structurée sous la forme de questions associées à une série de réponses de grande qualité. L’enjeu pour Quora est d’attirer des contributeurs de qualité et de les motiver à écrire d’excellentes réponses aux questions posées.

Pour atteindre ce but, Quora a fait un choix très clair :personnaliser les réponses, dépersonnaliser les questions.Les questions ne sont pas visuellement attachées à celui qui les a posé. Elles sont immédiatement traitées comme un bien commun. Chacun peut les modifier et les améliorer. Dès qu’un utilisateur pose une question, il ne la contrôle plus.

En revanche tout dans l’interface reinforce le lien entre une réponse et son auteur. Dans Quora, la juxtaposition systématique de l’identité de l’utilisateur, mentionnant en particulier sa photo, son nom et sa bio, et de ses contributions introduit une équivalence directe entre la valeur d’un utilisateur et la qualité de ses réponses. Quora propose ensuite un système de classement explicite : les réponses les mieux notées sont présentées en premier juste en dessous de la question. Chaque question est donc une simplement une compétition entre utilisateurs. Celui qui propose la meilleure réponse gagne le jeu. Quora se contente ensuite de documenter au mieux cette compétition en fournissant des outils qui permettent aux utilisateurs de suivre en temps réel les performances de leur réponse dans chacune des compétitions dans lesquelles elles sont engagées.
Comme dans Twitter, le contributeur de Quora découvre ces règles au fur et à mesure et apprend ce qu’il doit optimiser pour gagner dans ce type particulier de compétition.

## Wikipedia est aussi un jeu

Il est clair que Wikipedia n’a pas adopté la même stratégie ludique que Quora ou que Twitter. Il n’en reste pas moins que sa structure et sa réussite sont la résultante, au moins autant sinon plus, de la qualité de sa construction ludique que du rêve partagé d’une encyclopédie universelle, libre et gratuite.

Quel jeu est alors Wikipedia ? La réponse est évidente. C’est un MMORPG  (« Massively Multiplayer Online Role Playing Games »). Les contributeurs les plus actifs de Wikipedia ont un démarche qui a beaucoup de points communs avec celle des joueurs de World of Warcraft.

Pour jouer à Wikipedia, il faut contribuer. Contrairement à Quora qui impose à l’utilisateur novice un long et fastidieux processus de formation avant de pouvoir commencer, le processus d’onboarding de Wikipedia est beaucoup plus doux. Pas de login. Pas besoin d’être identifié. Le fait que l’historique de chaque page est conservé et qu’il soit toujours possible de revenir à une ancienne version d’un article permet cette ouverture. Mais la force conceptuelle de ce principe d’ouverture (Le projet d’encyclopédie libre que vous pouvez améliorer) ne doit pas masquer les vrais dynamiques qui pousse les utilisateurs à s’investir dans Wikipedia.

Après quelques contributions ou corrections anonymes, il est naturel que l’utilisateur souhaite signer ses ajouts. En s’identifiant, le Wikipedien développe une identité propre avec nom, une page personnelle. Il vient, peut-être sans le savoir, de passer au niveau 2.

Comme dans tout bon jeu, cette étape n’est que la première d’une longue série qui permettra à l’utilisateur motivé de franchir les niveaux successifs de la grande pyramide Wikipedia. Comment gravit-on les échelons ? Quels sont les privilèges reservés à ceux qui sont plus hauts ? Pourquoi certaines page sont-elles protégées contre les modifications ? Qui peut décider de ces choix ? Le novice ne le sait pas encore.

Wikipedia n’est pas aussi transparente que son mythe fondateur le laisse entendre. Comprendre ses rouages, ses comités, le système d’attribution de ses privilèges fait partie du plaisir de la découverte du joueur qui veut explorer ce monde finalement peu connu, réservé à quelques initiés qui ont fait l’effort de s’y investir.  Pour le novice, l’encyclopédie « participative » semble au début cacher relativement bien ses secrets.  La meilleure, et peut-être la seule, manière de comprendre Wikipedia est d’y jouer longtemps.

## Les classes de personnages

Au fur et à mesure qu’il réside dans Wikipedia, le joueur apprend les différents métiers qu’il pourrait exercer dans ce monde : administrateur, bureaucrate, steward, médiateur, arbitre, masqueur, importateur, vérificateur d’adresse IP. Comme dans tout jeu de rôle qui se respecte, chaque caste a ses devoirs et ses pouvoirs spéciaux.

Les administrateurs assurent typiquement la maintenance (nettoyage) de certaines pages, vérifier que les contenus ne posent pas de problèmes de droits d’auteur, réparer les actes de vandalisme. Toutes ces opérations peuvent être effectué par n’importe quel contributeur mais les administrateurs ont aussi accès à des pouvoirs supplémentaires :effacer des pages non pertinentes ou au contraire les « protéger » (empêcher leur modification), bloquer certains utilisateurs, renommer des pages, masquer des versions de l’historique.

Comment devient-on administrateur ? Il faut être élu. La page correspondante indique les critères suivants non obligatoires mais recommandés  : « une bonne connaissance de la syntaxe wiki, des règles et du fonctionnement de Wikipédia en français, une participation au minimum à des travaux du Projet:Maintenance, environ 3 000 contributions et un an d’activité significative ». La route est longue, comme dans tout bon jeu de rôles.
Pour réussir sa campagne et se faire élire, il est important de comprendre le processus de vote et de décision.  Seul les votes des contributeurs ayant 50 contributions significatives à leur actif sont pris en considération. La candidature dure quinze jours. Si les votes sont favorables à l’accès au statut, le candidat est nommé. Les instructions précisent cependant que « La définition d’un vote « favorable » relève du pouvoir discrétionnaire des bureaucrates« .

Déjà notre administrateur voit plus haut et plus loin. Un jour peut-être il sera lui aussi « bureaucrate ». Les bureaucrates sont chargés de gérer les statuts de certains contributeurs à Wikipedia en particulier les administrateurs mais aussi les bots, ces algorithmes qui contribuent à Wikipedia effectuant des tâches répétitives et fastidieuses pour un humain (gestion des liens d’interlangue, la résolution des homonymies, les annulations de certainsvandalismes). Environ de 8 personnes ont ce privilège sur Wikipédia en français.

Les stewards sont des « super bureaucrates ». En plus de gérer le statut des administrateurs, des bots et des bureaucrates, ils nomment également les masqueurs (ceux qui peuvent cacher des parties de Wikipedia comme des pages, des commentaires ou des historiques) et les vérificateurs d’adresses IP (qui peuvent faire le lien entre un compte utilisateur et l’adresse IP). Il n’y a que 3 stewards sur Wikipedia en Français.

Il faudrait encore parler des médiateurs, capable de s’interposer dans les disputes mais qui n’ont pas le pouvoir de voter ou de recommander une action punitive,  et des arbitres qui eux peuvent imposer une décision définitive. Le prestigieux ArbCom (Arbitration Committee) de la version Anglaise de Wikipedia n’a qu’une quinzaines de membres.

Wikipedia a aussi ses histoires fondatrices. Une des plus célèbres est la controverse sur Essjay, membre éminent de la Wikicratie qui cumulait les fonctions d’administrateur, bureaucrate, arbitre et mediateur, et qui fut pris en flagrant délit de mensonge sur sa page Wikipedia. 

## L’envers du décor

Jouer le jeu de Wikipedia c’est faire un voyage initiatique dans l’envers du décor. Comprendre que la plus grande encyclopédie en ligne n’est pas tout à fait le simple résultat de la « sagesse des foules » mais a su s’auto-organiser autour d’une bureaucratie émergente. Wikipedia est véritablement un monde en soi avec sa politique, son histoire, son oligarchie.

Ainsi, World of Warcraft semble presque ennuyeux comparé aux mystères que l’exploration patiente de Wikipedia peut révéler. Il semble évident que. par bien des aspects, s’investir dans Wikipedia est une activité similaire à faire progresser son personnage dans un jeu de rôle multijoueurs. Et sur Wikipedia comme dans le MMPORG les joueurs avancent le plus souvent masqué derrière un pseudo qui leur permet de véritablement vivre une double vie. Employé du bureau le jour, Wikipedien la nuit.  Avec temps et persévérance, le contributeur se construit une identité propre, avec des pouvoirs associés, aussi difficilement acquis que ceux qui vous permettent d’être grand mage dans un univers d’heroic fantasy.
Tout cela n’enlève rien au fantastique édifice que Wikipedia représente, à la valeur de cette oeuvre collective, à la manière dont ce service gratuit à changer nos vies. Mais pour comprendre comment Wikipedia a pu croitre et prospérer, il faut réaliser, qu’avant tout, Wikipedia est un jeu.



# Comment composer des séries télévisées continuellement intéressantes ?

Pour ce chapitre, je m’appuie sur une série d’études que nous avons menées sur les séries télévisées démontrant que leur complexité narrative n’a fait que s’accroitre au fil des années. Je discute la particularité de ce mode de récit et plaide pour l’émergence d’une narratologie des récits ouverts (capables d’étudier des histoires dont on ne sait jamais vraiment quand elles vont finir). Je m’interroge sur le rôle que jouent ces nouveaux récits dans le cadre de la crise narrative qui caractérise les dix premières années du XXIe siècle. M’appuyant sur une analyse de l’histoire du roman et sur le fait que les formes populaires annoncent presque toujours l’arrivée de formes savantes, je conclus sur ce que pourrait être une nouvelle forme de communication scientifique basée sur les principes narratifs de séries télévisées. 

# Comment enseigner à 10 000 élèves ?

Dans ce chapitre, je reviens sur l’histoire récente des MOOCs (Massive Open Online Courses) et m’interroge sur les transformations que ces nouvelles formes d’enseignements introduisent dans nos systèmes éducatifs. Je montre au travers de nos propres expériences à l’EPFL comment il est possible d’enseigner simultanément à 10 000 élèves et comment ce type de formation permet de donner un rapport complètement différent à la construction de la connaissance collective. Je m’interroge ensuite sur la façon dont ce type d’enseignement redessine le paysage éducatif global. 

# Comment faire des musées et des bibliothèques des lieux de découverte quand tout est déjà sur Internet ?

À l’heure de l’Internet, pourquoi aller encore au musée ou en bibliothèque ? Dans ce chapitre, je reviens sur l’évolution des bibliothèques et sur leur rôle social dans la découverte des livres. J’argumente que la bibliothèque n’a plus de raison d’être un lieu qui conserve des livres mais doit se transformer toute entière en une interface de découverte. Je montre la pertinence potentielle de ce type de lieu comme un espace de nouvelles sociabilités. Je reviens également sur l’histoire des procédés muséographiques et la manière de construire une médiation efficace en utilisant les nouvelles technologies. Je m’interroge sur la place du musée à l’heure des l’Internet et des écrans et plaide pour une meilleure utilisation de l’espace physique. Je développe l’idée de procédés en réalité mixte et, en me basant sur notre propre expérience, argumente en faveur du retour des projections fantasmagoriques dans les musées. 

# Comment étudier les centaines de millions de photos postées chaque jour sur les réseaux sociaux ?

Dans ce chapitre, j’explore le potentiel pour le sciences humaines et sociales du flux de photographie postée chaque jour sur les réseaux sociaux. Je reviens sur l’histoire des pratiques qui lient photographie et sciences humaines. Je montre ensuite comment, grâce aux algorithmes de reconnaissance de visages et de lieux, des analyses sociologiques peuvent être menées et discute les questions liées à la protection de la sphère privée dans ce type de recherche. J’évoque la densification panoptique engendrée par l’arrivée de Google Glass et montre les potentiels et les périls de ces nouveaux régimes de visibilité. 

>> Chapitre Tableau ?


# Comment vivre dans le grand maintenant ?

Dans ce chapitre conclusif, je reviens sur les grandes transformations temporelles qui ont caractérisé les quinze dernières années et m’interroge sur la manière de vivre dans le grand maintenant, c’est-à-dire à une époque caractérisée par un présent infiniment dense et des horizons temporels ayant tendance à se rétrécir. Je m’interroge sur le rôle nouveau de la sociologie pour comprendre cette société connectée et la manière dont une sociologie just-in-time peut se constituer à partir de l’analyse des réseaux sociaux. Je reviens enfin sur plusieurs des thèmes de ce livre, car une telle sociologie doit nécessairement embrasser les autres dimensions culturelles (linguistique, littéraire, etc.) de son époque. 

# Comment conserver tous les savoirs du monde. 





# Comment  organiser tous les savoirs du monde. 

# Comment  sauvegarder le passé pour l’humanité / Comment le passé survit / Copier-coller. 

##  Facsimilé

Numérisation massive. Ce ne sont que des facsimilé. Impression 3D. Les grilles de Versailles. 

Chaine de reproduction des copistes / Scriptorium massif d’Eusebe 
Chaine de reproduction du theatre


##  Fondation CINI / Veronese

Venise, 1791 Napoleon. Histoire d’un viol patrimonial 

Venise, 2007 Fondation CINI. 

Qu’est ce qui se transmets



# Comment  survivre au déluge informationel

##  Un vieux problème

##  Exposé sur le journal et l’encyclopedie

##  La recherche dans les archives

Fillipo di Vivo — Première description de l’archive d’etat 
L’archives espaces en 3 dimensions contenant des conteneur. 

Les index








## Conservation

Numeriser = Conserver ?
Conserver = Reproduire dans le temps
Latour

Les gens vont encore voir la Tour Effeil. 

# Que faire de 10 millions de photos par heure

Lien avec metamorphose des objets
Histoire de la vie privée
Reprendre toute la partie deja écrite

 # Doit-on se rappeler de tout ?

Droit a l’oubli, devoir de mémoire

- L’age de l’indulgence
- L’age de la mascarade (carnalva de Venise)
- L’age du panoptique / Société Victorienne

# Comment créer un produit sans argent et sans employers 

## Comment faire transcrire un manuscrit vénitien aux philippines ?

Crowdsourcing 

# Comment ne pas oublier les travailleurs de l’ombre ?
- Google Books (Art of Google Books / Google Hands)
- Censure
- 